<?xml version="1.0" encoding="utf-8"?>
<search>
  
    <entry>
      <title><![CDATA[intellij에서 python interpreter 셋팅하기]]></title>
      <url>/python/2020/03/21/setting-python-interpreter-in-intellij/</url>
      <content type="text"><![CDATA[인텔리제이 울티메이트(유료버전)에서 파이썬 프로젝트를 구동하기 위한 인터프리터 셋팅 방법preferences -&gt; plugins에서 python을 검색 후 install 후 intellij를 재시작한다.file -&gt; product structureplatform settings -&gt; SDKs 에서 + 클릭 후  Python SDK 클릭그럼 이렇게 interpreter를 추가할 수 있다. virtualenv가 이미 있다면 Existing environment 체크 후 만들어 놓은 virtualenv를 선택하면 되고,만약 새로운 virtualenv를 생성해야한다면 New environment 체크 후 Location에서 virtualenv 파일이 생길 경로를 지정해주고 Base interpreter에서 python 버전을 선택해준다.virtualenv 설정이 끝나면 OK그럼 SDKs에 interpreter가 추가되어있을 것이다.SDKs에 위에 packages 탭을 누르고 +를 눌러 패키지를 설치할 수 있다.검색창에 설치할 패키지를 검색 후 Install package를 눌러 패키지를 설치한다.만약 프로젝트에 requirements.txt가 있으면 terminal에서 pip install -r requirements.txt 로 패키지를 설치한다. (새로 virtualenv를 만든 경우엔 requirement로 설치하는 방법을 못찾았다ㅠ 아시는 분 댓글 부탁드립니다)그 후에 다시 패키지 목록이 보이는 곳에서 apply 후 OK 를 눌러주면 virtualenv 설정과 interpreter 추가 끝.이제 프로젝트에 interpreter를 등록해주면 된다.project -&gt; project sdk에서 추가한 interpreter를 등록해준다.추가로 필요에 따라 실행 시킬 파일에 인터프리터를 셋팅하고 싶으면 Run -&gt; Edit configuration에서 인터프리터를 등록해주면 된다.그럼 끗 😜]]></content>
      <categories>
        
          <category> python </category>
        
      </categories>
      <tags>
        
          <tag> intellij python project </tag>
        
          <tag> how to setting lntellij python interpreter </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Expecting property name enclosed in double quotes 해결]]></title>
      <url>/python/2020/02/04/python-json-load-error-solution/</url>
      <content type="text"><![CDATA[python에서 json으로 load시에 아래와 같은 에러가 발생하는 경우가 있다.JSONDecodeError: Expecting property name enclosed in double quotesjson 데이터는 더블 쿼터(double quotes)만 사용할 수 있기 때문에 아래와 같이 프로퍼티를 감싼 게 싱글 쿼터(single quote)일 시 에러가 난다.import jsonjson_data = "{'key': 'value', 'key2': 'value2'}" # ERROR// json_data = '{"key": "value", "key2": "value2"}' # OK json.loads(json_data)data를 직접 만드는 경우라면 더블 쿼터로 바꿔주면 그만이지만 request, response 받은 데이터는 변경도 못한다.그럴땐 아래와 같이 해결할 수 있다.import astdata = ast.literal_eval(data)ast.literal_eval()을 사용하면 위 에러를 해결하고 json -&gt; dict 타입으로 변환할 수 있다.referhttps://stackoverflow.com/questions/25707558/json-valueerror-expecting-property-name-line-1-column-2-char-1]]></content>
      <categories>
        
          <category> python </category>
        
      </categories>
      <tags>
        
          <tag> Expecting property name enclosed in double quotes </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[git reset과 revert]]></title>
      <url>/til/2019/09/23/TIL-git-origin-commit-cancel/</url>
      <content type="text"><![CDATA[TIL 카테고리의 글은 그날 배운 것을 정리하는 목적으로 포스팅합니다. 내용이 잘못되었다면 댓글로 피드백 부탁드립니다.원격 저장소에 올라간 커밋을 되돌리는 방법은 2가지가 있다.로컬에서 커밋을 되돌린 후 강제로 force push 하는 방법git reset 명령어를 사용해서 커밋을 되돌린다.예시로 아래 보이는 comit: C 커밋을 취소해보자.git reset --hard HEAD~1위 명령어는 커밋을 되돌리는데, 커밋한 코드도 다 날아가기때문에 조심해야한다.로컬 저장소에서 커밋을 취소했기 때문에 원격에도 반영을 시켜주기 위해 push를 해줘야하는데 현재 로컬 저장소의 히스토리가 원격 저장소의 커밋 히스토리보다 뒤쳐져 있기 때문에 push할 경우 에러가 발생한다. 그렇기 때문에 이러한 상황에서는 force push를 해줘야한다.git push -f origin &lt;branch-name&gt;주의 사항  이 방법은 내 로컬 저장소의 히스토리를 원격 저장소 히스토리로 강제 덮어쓰기 하는 것이므로 원격 저장소에 흔적도 없이 내가 만들었던 커밋을 제거한다.  만약 해당 브랜치가 혼자 사용하는 브랜치가 아닌 팀원들과 공유하는 브랜치이고 커밋을 되돌리기 전, 원격에 올라간 내 커밋을 다른 팀원이 pull 땡겼을 경우 원격 저장소에서 커밋을 되돌려도 다른 팀원의 로컬 저장소에는 커밋이 남아있게 된다. (이 상황에서 내가 원격 저장소에 force push 한 것을 팀원이 pull을 해도 origin/master의 커밋으로 돌아가지 않는다. origin/master보다 새로운 커밋을 가지고 있는 것으로 인식하기 때문)  커밋을 되돌려진 것을 모르고 팀원은 자신이 작업한 커밋과 되돌린 커밋을 push하게 되면 되돌렸던 커밋들이 다시 원격 저장소에 추가됨. 🤦‍♀따라서 reset + force push 방법은  나혼자만 사용하는 브랜치에 커밋을 push 했고, 이를 되돌리고 싶은 경우  팀원들과 커뮤니케이션을 통해 내가 되돌린 커밋을 pull로 땡겨간 팀원이 없다고 확인된 경우에만 사용해야한다. (왠만하면 공용 브랜치에는 사용하지 않는다)Revert를 통해 커밋 되돌리는 방법git revert 명령어는 revert 커밋을 히스토리에 쌓는 방식이다.reset과의 차이점은 reset은 커밋을 되돌리면서 해당 커밋 이력은 사라진다, revert는 커밋을 되돌리는 커밋을 찍어커밋과 커밋을 되돌린 이력이 모두 남는다. 특정 커밋을 되돌리는 작업도 하나의 커밋으로 간주하는 것이다.reset을 사용하면 커밋을 되돌려서 push 했기 때문에 원격-로컬간의 히스토리가 맞지 않아 에러가 나고 팀원들의 커밋과 꼬일 가능성이 큰데 이건 되돌리는 커밋을 하나 더 생성하기 때문에 에러도 나지 않고 커밋이 꼬일 확률이 적다.revert를 할 때에는 커밋을 한 순서 거꾸로 revert를 해야한다. (가장 나중에 한 커밋을 제일 먼저 revert 해야함)# git revert &lt;commit hash&gt;# 여러개의 커밋을 한꺼번에 revert할 경우 : git revert HEAD~3git revert 1347972위 명령어를 사용하면 1347972 commit을 되돌리는 커밋이 찍힌다.--no-commit 옵션을 사용하면 자동으로 revert 커밋이 생성되는게 아닌 working tree와 index에만 변경사항이 적용된다. 이걸 사용해 여러개의 커밋을 하나의 커밋으로 revert 할 수 있다.git revert --no-commit HEAD~3 # revert 커밋이 자동으로 생성되지 않음.git commit -m "Revert Commit C, B, Agit push origin master나는 실제로 팀원들과 협업하여 개발할 때에는 안전한 Revert를 이용한 되돌리기를 사용한다.정리  나름 정리한거다referhttps://behonestar.tistory.com/200https://jupiny.com/2019/03/19/revert-commits-in-remote-repository/]]></content>
      <categories>
        
          <category> TIL </category>
        
      </categories>
      <tags>
        
          <tag> git </tag>
        
          <tag> git revert </tag>
        
          <tag> git reset </tag>
        
          <tag> git force push </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[파이썬 사용해서 엑셀 파일 만들기]]></title>
      <url>/python/2019/09/04/the-meaning-of-underscores-in-python/</url>
      <content type="text"><![CDATA[이 글은 The Meaning of Underscores in Python 번역한 글입니다. 오역 발견시 댓글 남겨주시면 정정하겠습니다. 🙇‍♀️파이썬의 단일 밑줄(_)과 이중 밑줄(__)에 대한 다양한 의미와 네이밍 컨벤션을 알아보고 이것이 파이썬 클래스에서 어떻게 동작하고 어떤 영향을 주는지 알아봅시다.단일 또는 이중 밑줄 이름의 파이썬 변수와 메소드에는 의미가 있습니다.]]></content>
      <categories>
        
          <category> python </category>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[파이썬 사용해서 엑셀 파일 만들기]]></title>
      <url>/til/2019/08/29/TIL-make-excel-with-python/</url>
      <content type="text"><![CDATA[TIL 카테고리의 글은 그날 배운 것을 정리하는 목적으로 포스팅합니다. 내용이 잘못되었다면 댓글로 피드백 부탁드립니다.엑셀 파일을 만들 수 있는 파이썬 라이브러리는 대표적으로  openpyxl 과 xlsxwriter 가 있는데 저는 wlsxwriter 를 사용했다. (보통 openpyxl을 더 쓰는 것 같은데 나는 wlsxwriter 예제를 처음 접해서 사용했다. )엑셀 파일 만들기아래 코드는 간단한 엑셀 파일을 만드는 코드이다.import xlsxwriterfile_name = 'test.xlsx' # 엑셀 파일 이름workbook = xlsxwriter.Workbook(file_name)ws = workbook.add_worksheet()ws.write(0, 0, 'Hello') # A1에 값을 넣음ws.close() # 저-장 같은 디렉토리에 test.xlsx 파일이 생성된 것을 확인할 수 있다.이미지 삽입하기image url로 requests를 통해 이미지를  가져온 뒤 이미지도 삽입되어있는 파일을 만들어 보았다.from io import BytesIO  import requestsimport xlsxwriterimage_url ='https://d3mcojo3jv0dbr.cloudfront.net/2019/03/23/14/25/d88f24f059169559602c836fc35b995c.jpeg?w=128&amp;h=128&amp;q=65'workbook = xlsxwriter.Workbook('text.xlsx')ws = workbook.add_worksheet()ws.set_column(0, 0, 100) # column width 설정 ws.set_column(1, 1, 100) # column width 설정 ws.set_default_row(100) # row height 설정ws.write(0, 0, '어피치') # A1 res = requests.get(image_url) # 이미지 가져옴image_data = BytesIO(res.content) # 이미지 파일 처리image_size = len(image_data.getvalue()) # 이미지 사이즈 if image_size &gt; 0: # 이미지가 있으면     ws.insert_image('B1', image_url, {'image_data': image_data}) # 엑셀에 삽입함workbook.close() # 저장 이미지 사이즈를 체크하여 이미지가 있는지 확인한 후 엑셀에 삽입하는 이유는 이미지가 없는데 inser_image를 시도하면 오류가 나기 때문이다.struct.error: unpack requires a bytes object of length 3 image(이 오류때문에 반나절 삽질했다 ㅂㄷㅂㄷ)  해당 오류 이슈이렇게 파이썬으로 엑셀 파일 만드는 것을 자동화하여 생산성을 높일 수 있다.referxlsxwriter 사용법xlsxwriter document엑셀 이미지 삽입하기 ]]></content>
      <categories>
        
          <category> TIL </category>
        
      </categories>
      <tags>
        
          <tag> python excel </tag>
        
          <tag> python make excel </tag>
        
          <tag> python excel insert image </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[광고 시스템]]></title>
      <url>/til/2019/08/28/TIL-ad-system/</url>
      <content type="text"><![CDATA[TIL 카테고리의 글은 그날 배운 것을 정리하는 목적으로 포스팅합니다. 내용이 잘못되었다면 댓글로 피드백 부탁드립니다.RTB  Real Time Bidding 자동화 거래 시스템리타겟팅 광고  유저가 봤던 상품과 같거나 비슷한 상품을 보여주는 것  효과가 제일 좋음.  유튜브에서 자신이 보던 동영상 밑에 빨간 타임라인이 보이는데 이것을 이용해 동영상 썸네일에 타임라인을 집어넣어 사용자가 자신이 봤던 동영상으로 알고 클릭하도록 유도하는 것도 비슷한듯.  페이스북 쿠팡 광고도 한번 봤던거 계속 광고함..Native Ad      유저가 지금 보고 있는 페이지에 나오는 상품과 매우 적합도가 유사한 상품을 보여주는 방식        뉴스 내용에서 에어프라이어가 나온다면 옆에 에어프라이어 배너를 보여주는 것.  CPM  Cost Per Mile1000번 노출당 광고 비용 지불.  광고가 클릭이 되지 않아도 매체는 매출이 보장됨CPC  Cost Per Click클릭당 광고 비용 지불.  사용자에게 광고가 클릭될 시 비용을 지불함.CPA  Cost per Action (Install)광고주가 원하는 액션 한번에 대한 광고비용  만약 앱을 광고한다면 한번 인스톨당 광고비용을 뜻함CTR  Click -Through Rate광고가 클릭된 횟수를 광고가 게재된(노출된) 횟수로 나눈 값  쉽게 노출 대비 클릭 량.  CTR이 높아야 광고가 잘 되고 있음을 뜻함Fill rate  광고 지면 대비 실제로 광고가 개제되는 수ROAS  Return On AD Spend광고비 지출에 대한 효과  내가 광고를 이만큼 했는데 얼만큼 효과를 봤는가?  광고비를 많이 지출할 수록 ROAS는 떨어진다.RTB 거래 입찰 모델Second Price Auction  제일 비싼 가격을 제안한 사람이 2번째로 높은 제시가로 입찰되는 것.  만약 1등이 500원, 2등이 400원을 제시했다면 1등은 400원을 내는 것. 2등은 3순위 제시가격, 3등은 4순위 제시가격..  1등이 2등의 제안가를 내는 것이 매체에겐 손해같지만 손해가 아님.Referhttp://blog.ab180.co/data-science-with-r-6-rtb/]]></content>
      <categories>
        
          <category> TIL </category>
        
      </categories>
      <tags>
        
          <tag> ad system </tag>
        
          <tag> RTB </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[git remote url 변경하기]]></title>
      <url>/til/2019/07/29/change-git-remote-url/</url>
      <content type="text"><![CDATA[TIL 카테고리의 글은 그날 배운 것을 정리하는 목적으로 포스팅합니다. 내용이 잘못되었다면 댓글로 피드백 부탁드립니다.상황사정이 생겨 github username을 변경하고 푸시를 하려고 했는데 authentication 오류가 나면서 푸시가 되지 않았다.fatal: Authentication failed for 'https://old-username@github.com/account/repo.git/해결usename을 바꾸면서 이전 username의 remote url을 사용할 수 없어 새로운 username이 들어간 remote url로 변경해줬다.$ git config --list # remote url 확인 remote.origin.url orgin https://old-username@github.com/account/repo.git/$ git remote set-url origin https://new-username@github.com/account/repo.git/]]></content>
      <categories>
        
          <category> TIL </category>
        
      </categories>
      <tags>
        
          <tag> git remote url 변경 </tag>
        
          <tag> change git remote url </tag>
        
          <tag> git username 변경 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[robots.txt는 뭘까?]]></title>
      <url>/til/2019/07/08/TIL-robots_txt/</url>
      <content type="text"><![CDATA[크롤링을 배우게 되면 여기 저기 사이트를 크롤링해서 데이터를 가져온 후 유용한 프로그램을 만들어 보고 싶을 것이다. 나도 크롤링을 배운 후 어디다 써보고 싶어서 이런 저런 아이디어를 생각해냈다. 그러다 문득 크롤링을 하는 것은 그 사이트의 데이터를 허가 없이 가져다 사용하는 것인데.. 크롤링을 통해서 수익을 내지 않더라도 웹 사이트나 어플을 런칭해도 될까?라는 생각이 들었다. 그래서 인터넷을 뒤져보기 시작했는데 지나가다 많이 보던 robots.txt 의 정체를 이제서야 알게되었다.robots.txt ?robots.txt 파일은 서버  root 디렉토리에 위치하는 파일이다. 이 파일을 통해 해당 사이트가 크롤링을 허용하는지 알 수 있다. google 의 robots.txt 파일을 보자.http://google.com/robots.txtUser-agent: *Disallow: /searchAllow: /search/aboutAllow: /search/staticAllow: /search/howsearchworksDisallow: /sdchDisallow: /groupsDisallow: /index.html?Disallow: /?Allow: /?hl=...user-agent : * 는 모든 유저에 대한 크롤링 권한을 뜻한다. Disallow에 해당하는 페이지는 크롤링을 허용하지 않는다는 것이고, Allow는 허용한다는 것이다. 구글은 검색 페이지에 대한 크롤링은 허용하지 않는 것 같다. 네이버도 알아보자https://www.naver.com/robots.txtUser-agent: *Disallow: /Allow : /$ 네이버는 모든 페이지에 대한 크롤링을 허용하지 않는다.이렇게 크롤링을 할 때 해당 사이트의 robots.txt을 통해 크롤링 허용 여부를 확인하고 프로그램을 개발하는 것이 좋을 것 같다!  또한 자신의 사이트를 만들 때에도 robots.txt 파일을 생성해 둠으로써 크롤링 허용에 대한 정의를 해두는 것이 좋을 것 같다.]]></content>
      <categories>
        
          <category> TIL </category>
        
      </categories>
      <tags>
        
          <tag> robots.txt </tag>
        
          <tag> 크롤링 불법 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Python sort와 sorted]]></title>
      <url>/python/2019/06/11/python-sort-sorted/</url>
      <content type="text"><![CDATA[list.sort()&gt;&gt;&gt; score = [92, 34, 56, 100, 80, 97, 20, 65, 70, 55]&gt;&gt;&gt; score.sort() # 오름차순&gt;&gt;&gt; score[20, 34, 55, 56, 65, 70, 80, 92, 97, 100]list.sort()는 리스트를 정렬하는데 기존 객체를 정렬한다. 따라서 반환 값이 없다.(None) score를 정렬 후 프린트 해보면 요소들이 오름차순으로 된 것을 볼 수 있다.sorted(list)&gt;&gt;&gt; score = [92, 34, 56, 100, 80, 97, 20, 65, 70, 55]&gt;&gt;&gt; sorted(score)[20, 34, 55, 56, 65, 70, 80, 92, 97, 100]&gt;&gt;&gt; score[92, 34, 56, 100, 80, 97, 20, 65, 70, 55]sorted(list)는 리스트를 정렬하는 것은 똑같지만 기존 객체를 정렬하는 것이 아닌 기존 객체의 요소를 정렬한 새로운 객체를 반환한다. 따라서 기존 객체인 score는 요소들의 위치가 변하지 않은 것을 확인할 수 있다.list.sort(), sorted(list)는 성능의 차이가 없으므로 둘 중 필요한 것을 사용하면 된다. 내림차순을 사용하려면 reverse=True 를 인자로 넘겨주면 된다.&gt;&gt;&gt; score = [92, 34, 56, 100, 80, 97, 20, 65, 70, 55]&gt;&gt;&gt; sorted(score, reverse=True) # 내림차순[100, 97, 92, 80, 70, 65, 56, 55, 34, 20]&gt;&gt;&gt; score.sort(reverse=True) # 내림차순&gt;&gt;&gt; score[100, 97, 92, 80, 70, 65, 56, 55, 34, 20]어떤 값을 기준으로 정렬하기여러개의 딕셔너리가 들어있는 리스트를 딕셔너리의 어떤 값을 이용해 정렬하고 싶다면 key 를 사용한다.&gt;&gt;&gt; users[{'age': 0}, {'age': 12}, {'age': 42}, {'age': 6}, {'age': 48}, {'age': 18}, {'age': 30}, {'age': 54}, {'age': 36}, {'age': 24}]&gt;&gt;&gt; users.sort(key=lambda x: x['age']) # age 값을 기준으로 오름차순 정렬&gt;&gt;&gt; users[{'age': 0}, {'age': 6}, {'age': 12}, {'age': 18}, {'age': 24}, {'age': 30}, {'age': 36}, {'age': 42}, {'age': 48}, {'age': 54}]&gt;&gt;&gt; users.sort(key=lambda x: x['age'], reverse=True) # age 값을 기준으로 내림차순 정렬&gt;&gt;&gt; users[{'age': 54}, {'age': 48}, {'age': 42}, {'age': 36}, {'age': 30}, {'age': 24}, {'age': 18}, {'age': 12}, {'age': 6}, {'age': 0}]만약 키 값이 있다 없다 한다면 dict.get()을 사용하자.&gt;&gt;&gt; users[{'age': 30}, {}, {'age': 6}, {'age': 24}, {'age': 48}, {}, {'age': 42}, {'age': 54}, {'age': 36}, {'age': 12}, {}, {}, {'age': 18}, {}]&gt;&gt;&gt; users.sort(key=lambda x: x.get('age', 0), reverse=True) # 키 값이 없으면 0&gt;&gt;&gt; users[{'age': 54}, {'age': 48}, {'age': 42}, {'age': 36}, {'age': 30}, {'age': 24}, {'age': 18}, {'age': 12}, {'age': 6}, {}, {}, {}, {}, {}]다중 조건으로 정렬하기만약 정렬을 하고 싶은 기준이 2개 이상이라면 아래 방법을 사용한다. (나이 오름차순 정렬 후,  점수는 내림차순으로 정렬)&gt;&gt;&gt; students[{'age': 0, 'score': 0}, {'age': 48, 'score': 1}, {'age': 40, 'score': 6}, {'age': 24, 'score': 8}, {'age': 16, 'score': 6}, {'age': 48, 'score': 2}, {'age': 64, 'score': 0}, {'age': 0, 'score': 3}, {'age': 56, 'score': 2}, {'age': 72, 'score': 6}, {'age': 72, 'score': 0}, {'age': 24, 'score': 5}, {'age': 8, 'score': 2}, {'age': 32, 'score': 1}, {'age': 16, 'score': 4}, {'age': 56, 'score': 9}, {'age': 32, 'score': 9}, {'age': 8, 'score': 1}, {'age': 40, 'score': 3}, {'age': 64, 'score': 2}]&gt;&gt;&gt; sort_students = sorted(sorted(students, key=lambda x: x['score'], reverse=True), key=lambda x: x['age'])&gt;&gt;&gt; sort_students[{'age': 0, 'score': 3}, {'age': 0, 'score': 0}, {'age': 8, 'score': 2}, {'age': 8, 'score': 1}, {'age': 16, 'score': 6}, {'age': 16, 'score': 4}, {'age': 24, 'score': 8}, {'age': 24, 'score': 5}, {'age': 32, 'score': 9}, {'age': 32, 'score': 1}, {'age': 40, 'score': 6}, {'age': 40, 'score': 3}, {'age': 48, 'score': 2}, {'age': 48, 'score': 1}, {'age': 56, 'score': 9}, {'age': 56, 'score': 2}, {'age': 64, 'score': 2}, {'age': 64, 'score': 0}, {'age': 72, 'score': 6}, {'age': 72, 'score': 0}]referhttps://stackoverflow.com/questions/4233476/sort-a-list-by-multiple-attributes]]></content>
      <categories>
        
          <category> python </category>
        
      </categories>
      <tags>
        
          <tag> python sort sorted </tag>
        
          <tag> 파이썬 sort sorted 차이 </tag>
        
          <tag> python sort key </tag>
        
          <tag> python sort key two </tag>
        
          <tag> python 다중 조건 정렬 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[python byte to string]]></title>
      <url>/python/2019/06/07/python-byte-to-string/</url>
      <content type="text"><![CDATA[파이썬 Bytes 타입을 str 타입으로 바꾸는 방법&gt;&gt;&gt; text = b'hello'&gt;&gt;&gt; type(text)&lt;class 'bytes'&gt;&gt;&gt;&gt; text.decode('utf-8')'hello'&gt;&gt;&gt; text = text.decode('utf-8')&gt;&gt;&gt; type(text)&gt;&gt;&gt; &lt;class 'str'&gt;]]></content>
      <categories>
        
          <category> python </category>
        
      </categories>
      <tags>
        
          <tag> python byte to string </tag>
        
          <tag> 파이썬 바이트 문자열 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[가비지 컬렉터]]></title>
      <url>/til/2019/06/06/TIL-garbage-collector/</url>
      <content type="text"><![CDATA[TIL 카테고리의 글은 그날 배운 것을 정리하는 목적으로 포스팅합니다. 내용이 잘못되었다면 댓글로 피드백 부탁드립니다.가비지 컬렉터   메모리 관리 기법 중의 하나로, 프로그램이 동적으로 할당했던 메모리 영역 중에서 필요없게 된 영역을 해제하는 기능C와 같은 언어들은 프로그램을 짤 때 메모리를 직접 관리해주어야 함. 변수나 함수를 사용할 때 더이상 사용하지 않는 변수나 함수의 메모리를 직접 해제해주는 등..만약 메모리를 비우지 않고 남긴 채로 계속 사용하다 보면 메모리 누수(memory leak)이 발생하게되고, 메모리가 꽉차게 되면 컴퓨터는터짐.이렇게 직접 메모리를 관리하지 않고 가비지 컬렉터가 메모리를 직접 관리해주는 언어들이 생김.  이렇게 메모리를 직접 관리해주는 언어를 managed language 라고하고 그렇지 않은 언어를unmanaged language라고함.가비지컬렉터는 특정 때에 필요없는 정보(garbage)를 버림.가비지 컬렉터 동작 방식  mark-and-sweep          메모리를 훑으면서 필요한 것들을 마크 후 마크되지 않은 것들을 버린다.        reference counting          한 요소가 다른 요소에게 몇번 참조되는 지 센 후 참조 번수가 0인 요소를 삭제하는 것.      메모리 누수를 방지하는 법      순환 참조를 하지 말자.                  변수끼리 서로 참조하는 것. 이렇게 되면 둘 다 참조 카운트가 0이 되지 않으므로 누수가 생김.        a = {}b = {}    a['b'] = bb['a'] = a     # NO!                     refer[얄팍한 코딩사전](https://www.youtube.com/watch?v=24f2-eJAeII )]]></content>
      <categories>
        
          <category> TIL </category>
        
      </categories>
      <tags>
        
          <tag> 가비지 컬렉터 </tag>
        
          <tag> garbage collector </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[DB - 인덱스]]></title>
      <url>/til/2019/05/18/TIL-DB-index/</url>
      <content type="text"><![CDATA[TIL 카테고리의 글은 그날 배운 것을 정리하는 목적으로 포스팅합니다. 내용이 잘못되었다면 댓글로 피드백 부탁드립니다.대용량 데이터에서 사용자 ID 1,234,567번의 데이터를 어떻게 빠르게 얻을 수 있을까?인덱스란  대용량 데이터에서 쉽게 원하는 데이터를 찾기위한 방법원하는 위치까지 빠르게 도달하는 방법  사용자 정보를 고정 길이로 관리함사용자 정보가 무조건 100바이트로 관리된다고 하면 사용자 ID * 100바이트 가 해당 사용자의 정보가 들어가있는 시작 위치가 됨. 1,234,567 사용자를 찾기 위해서도 1,234,567 * 100 을 하면 원하는 데이터를 빠르게 얻을 수 있음.단 무조건 100바이트 값이기 때문에 100바이트 이상이 되는 데이터를 저장하지 못함. 그렇다고 넉넉하게 10000바이트로 한다고 하면 낭비가 너무 심함.따라서 고정 길이 파일로 관리하는 방식은 좋지않음.  인덱스 구조 도입고정 길이 방식이 아닌 가변 길이 방식으로 데이터를 관리함. 예를 들어 책의 색인처럼 키워드와 기재된페이지로 구성되어있도록.소프트웨어 139쪽이것을 응용하여 사용자 정보 데이터가 저장되는 곳가 별도로 사용자 ID(키워드)와 사용자 정보가 저장된 위치를 저장하는 파일을 만들어 사용자 정보를 빠르게 얻을 수 있음.  사용자 ID와 저장된 위치(주소)는 숫자로 관리하면 고정 길이로 취급할 수 있기 때문에 빠름.이렇게 인덱스 구조를 도입하면 사용자 정보 데이터가 저장될 때 인덱스에도 새로운 데이터의 위치를 업데이트 해야하기 때문에 SELECT(검색)의 속도는 빨라지나 그 외 INSERT, UPDATE, DELETE 작업은 느려짐.해시 인덱스데이터의 키 값은 숫자가 아닌 문자열, 날짜가 될 수 도 있으므로 앞서 말했듯이 고정 길이로 취급하는게 어려울 때가 있음. 그래서 실제로는 키 값을 해시 함수에 대입해서 해시 값으로 저장하는 구조가 많이 사용됨. 해시는 문자열 길이에 상관 없이 동일한 크기이기때문에 고정 길이 포맷으로 대응할 수 있음. 해시 계산 비용도 O(1) 이므로 엄청 빠름!하지만 해시 인덱스는 범위 검색에서는 뛰어나지 못함. id =  1인 데이터를 가져오는 것은 빠를 지 몰라도 created_at &gt; 2019-01-02와 같은 범위 검색에선 사용할 수 없음.  아래 목적에서는 사용 불가능  price &lt; 10000  title LIKE 'FIMAL%'  ORDER BY created_at DESC**해시 인덱스는 많은 검색 작업에서 단지 일부 용도에서만 빠르게 처리할 수 있음. **B+Tree 인덱스나무 구조로 되어있는 인덱스. 정상에 있는 블록이 ROOT 블록 , 최하층이 Leaf 블록, 중간에 있는 Branch 블록.루트 블록과 브랜치 블록은 검색의 키인 사용자 ID에 대해 해당 블록이 어디에 있는지 정보를 가지고 있음. 최하층의 리프 블록에는 실제 데이터의 저장 위치의 정보를 가지고 있음.인덱스 검색 시 루트 -&gt; 브랜치 -&gt; 리프 순으로 도달해 원하는 데이터를 얻을 수 있음.레코드 수가 적으면 루트와 리프만 있는 패턴도 존재함. 레코드수가 많으면 브랜치 아래에 브랜치가 들어가있는 4계층 이상의 구성이 될 수 도 있음.최적화  고유성 보장하기          인덱스는 고유성을 보증하기 위한 목적으로 사용가능함. 해시 인덱스는 동일한 ID의 경우 동일한 해시 값이 되고 B+Tree 인덱스에선 동일 리프에 도달하기 때문에 적은 코스트로 쉽게 중복체크가 가능함. 따라서 고유성을 보장하려는 열(PK, Unique key)에 인덱스를 지정하는 것이 좋음. (필수로 되어있기도 함)        멀티 칼럼 인덱스          여러개의 조건을 지정해 검색하고 싶은 경우를 위해 여러 조건의 인덱스를 걸어 검색을 가속화 할 수 있는데 이것을 멀티 칼럼 인덱스라고 함.        Index only read          COUNT() 함수와 같이 레코드의 값이 아닌 레코드 개수를 구하고 싶은 경우에는 인덱스만 읽어서 결과를 구할 수 있음. 데이터 영역을 읽지 않고 인덱스 영역만 읽어 처리를 빠르게 할 수 있음.      refer웹 프로그래머를 위한 데이터베이스를 지탱하는 기술]]></content>
      <categories>
        
          <category> TIL </category>
        
      </categories>
      <tags>
        
          <tag> db index </tag>
        
          <tag> index </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[트랜잭션]]></title>
      <url>/db/2019/05/05/DB-transaction/</url>
      <content type="text"><![CDATA[트랜잭션은 데이터베이스에서 중요한 개념이다. 나 또한 공부하면서 많이 접했지만 도통 이해하기가 힘들었다. 실무를 시작하고 나서 왜 트랜잭션이 중요한지 깨닫게 되었고, 예전의 나처럼 실무를 접해보지 않아 트랜잭션이 무엇이고 왜 중요한지 이해가 되지 않는 사람들을 위해 이 글을 작성해보려고 한다.  트랜잭션(Transaction)은 데이터베이스의 상태를 변환시키는 하나의 논리적 기능을 수행하기 위한 작업의 단위 또는 한꺼번에 모두 수행되어야 할 일련의 연산들을 의미한다.위의 정의를 좀 쉽게 풀어보자면  **데이터베이스의 상태를 변환시키는 하나의 논리적 기능을 수행하기 위한 작업의 단위 **는 DML 즉 INSERT, UPDATE, DELETE, SELECT  들을 알맞게 사용해서 하나의 작업을 구현하는 것이라고 생각하면 된다. 또한 이 쿼리를 실행할 때 모두 성공(수행)해야 작업이 정상적으로 처리됨 뜻한다.어떤 하나의 기능을 구현하게 해주는 쿼리들 중에서 한개의 쿼리가 실패했다면 이 전의 쿼리들은 rollback 되어야 하고, 모든 쿼리가 성공했다면  commit 을 해줘야한다.  Commit과 Rollback 본인이 어떤 은행의  계좌이체 서비스를 직접 개발한다고 생각해보자. 상황은 A가 친구 B에게 5만원을 이체하려고 한다.현재 A 계좌에는 10만원이 있고 B는 3만원이 있다. 만약 이체가 성공적으로 끝나게 되면 A 계좌에는 5만원, B 계좌에는 8만원이 남아야한다.위의 로직을 짜보면 먼저 A  잔액을 SELECT 한 후 이체하려는 금액  5만원이  잔액보다 적은지, 큰지 확인 후 잔액이 크다면 이체를 할 수 있을 것이다. 그러면 A 계좌 잔액 - price 해서 나온 금액 을 UPDATE 한다SELECT money FROM account WHERE name = 'A';UPDATE account SET money = money - 50000 WHERE name = 'A' 이후  B 의 통장 잔고 잔액 + price 를 UPDATE 한다.UPDATE account SET money = money + 50000 WHERE name = 'B'위 시트를 보면 계좌이체가 성공했음을 볼 수 있다. 트랜잭션의 모든 쿼리가 성공했으니 commit을 하면 된다.여기서 계좌이체 트랜잭션을 A 계좌  SELECT 부터 B 통장 잔고 UPDATE 까지 잡는다는 것을 알 수 있다.  A 계좌 SELECT  A 계좌 UPDATE  B 계좌 UPDATE이렇게 트랜잭션을 잡은 이유는 이 쿼리들이 모두 성공해야 계좌이체가 성공하게 되기 때문이다. 이 트랜잭션에서 하나의 쿼리가 실패하게 된다면 작업은 아예 실패가 되어야 한다.만약에 A 계좌 UPDATE는 성공하고 B 계좌 UPDATE는 실패했는데 작업이 정상적으로 처리된다면 A는 돈을 입금했음에도 불구하고 B 계좌에는 돈이 들어오지 않아 B가 A에게 왜 돈을 안보내냐고 짜증을 내며 전화를 할 것이다. 그리고 A 와 B는 은행 서비스에 신뢰가 떨어지게 되고 결국 해당 은행은 망하게 될 것이다.UPDATE 부분은 이해가 가는데 왜 SELECT 부분도 같이 잡아야 할까? 이유는 A가 계좌이체 할 동안 다른 사람이 A 계좌에 접근하는 것을 막아야 하기 때문이다. A 계좌 SELECT 해서 10만원이 남아있다는 것을 확인하고 5만원을 빼려고 하는데 UPDATE 하기 전, 즉 SELECT와 UPDATE 사이에 C가 A 계좌에 2만원을 이체한다면 어떻게 될까? 현재 프로그램은 A의 계좌 잔액을 10 - 5 인 5만원으로 UPDATE 할 것이다. 하지만 이것은 잘못되었다. C가 2만원을 입금했으니 남은 잔액은 7만원이 되어야 한다. 이러한 문제를 막기 위해서 SELECT 까지 한꺼번에 트랜잭션으로 잡아 다른 사람이 A의 계좌에 접근하지 못하게 막아놔야 한다.이렇게 트랜잭션은 중요하다. 이제부터 기능을 만들 때 어떻게 트랜잭션을 잡아야 잘했다고 소문이 날 지 고민해보자!]]></content>
      <categories>
        
          <category> DB </category>
        
      </categories>
      <tags>
        
          <tag> DB 트랜잭션 </tag>
        
          <tag> db transaction </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[flask SQLAlchemy ORM 사용해보기]]></title>
      <url>/til/2019/04/19/TIL-flask-sqlalchemy-orm/</url>
      <content type="text"><![CDATA[TIL 카테고리의 글은 그날 배운 것을 정리하는 목적으로 포스팅합니다. 내용이 잘못되었다면 댓글로 피드백 부탁드립니다.ORM 사용법SELECTsession.query(Model).all() # SELECT * FROM modelsession.query(Model.id, Model.name, Model.age).all() # SELECT id, name, age FROM modelsession.query(Model).first() # SELECT * FROM model LIMIT 1from sqlalchemy import func # count 함수session.query(func.count(Model.id))# SELECT COUNT(id) FROM model# AS session.query(Model.id.label('model_id')).all()# SELECT id AS model_id FROM modelWHEREfilter를 사용한다.session.query(Model).filter(Model.name == 'lowell').all() # SELECT * FROM model WHERE name = 'lowell'session.query(Model).filter(Model.name == 'lowell', Model.age == 20).all() # SELECT * FROM model WHERE name = 'lowell' AND age = 20from sqlalchemy import or_ # OR 연산자session.query(Model).filter(or_(Model.mame == 'lowell', Model.age == 20)).all()# SELECT * FORM model WHERE name = 'lowell' OR age = 20INSERTuser = Model(name='lowell', age=20)session.add(user)session.commit() # session.rollback()# INSERT INTO model(name, age) VALUES ('lowell', 20)# COMMITUPDATE방법 1user = session.query(Model).filter(Model.name == 'lowell').first()user.age += 1session.commit()# select 후 update 함방법 2user = session.query(Model).filter(Model.name == 'lowell').update({'age': User.age + 1});session.commit()# select를 하지 않는 update 방법# UPDATE model SET age = age + 1 WHERE name = 'lowell'DELETEuser = session.query(Model).filter(Model.name == 'lowell').first()session.delete(user)session.commit()ORDER BYsession.query(Model).filter(Model.name == 'lowell').order_by(Model.created_at)# SELECT * FROM model WHERE name = 'lowell' ORDER BY created_atsession.query(Model).filter(Model.name == 'lowell').order_by(Model.created_at.desc(), Model.status) # SELECT * FROM model WHERE name = 'lowell' ORDER BY created_at DESC, statusJOININNER JOINsession.query(Model1, Model2).filter(Model1.id == Model2.id).all() # SELECT * FROM model1 JOIN model2 ON model1.id = model2.idsession.query(Model1).join(Model2, Model1.id == Model2.id).all()# SELECT * FROM model1 JOIN model2 ON model1.id = model2.idOUTER JOINsession.query(Model1). \    outerjoin(Model2, Model1.id == Model2.id).\    all()    # SELECT * FROM model1 LEFT JOIN model2 ON model1.id = model2.id 여러개 조인을 하기 이해선 그냥 이어 붙이면 된다.session.query(Model1.name, Model2.student_id, Model3.account).\	outerjoin(Model2, Model1.id == Model2.id).\	outerjoin(Model3, Model1.id == Model3.id).\	all()	# SELECT model1.name, model2.student_id , model3.account # FROM model1# LEFT JOIN model2 ON model1.id = model2.id# LEFT JOIN model3 ON model1.id = model3.idSELF JOIN  하나의 테이블을 조인하는 것aliased 를 사용하면 된다.model2 = aliased(Model)self.session.query(Model).\    join(model2, model2.id == Model.id).\    all()    # SELECT model.* FROM model JOIN model model2 ON model2.id = model.id;    GROUP BYsession.query(Model).group_by(Model.id).all()# SELECT * FROM model GROUP BY idSUBQUERYfrom sqlalchemy import subquerystmt = query.session(Model2).filter(Model2.grade == 'A').subquery() # SELECT id, grade FROM model2 WHERE grade = 'A'session.query(Model1, stmt.c.id, stmt.c.grade).\	outerjoin(stmt, stmt.c.id = Model1.id)	# SELECT model1.*, model2.id, model2.grade# FROM mode1l LEFT JOIN (SELECT id, grade FROM model2 WHERE grade = 'A') model2 # ON model1.id = model2.id그 외 팁CASE 문from sqlalchemy import case	session.query(	case(        	[            	(Model.age &gt;= 20, 'adult'),                (Model.age &gt;= 10, 'teenager')        	],         	else_='not adult, not teenager'    	)    ).\    filter(Model.sex='female').\    all()# SELECT CASE WHEN age &gt;= 20 THEN 'adult' WHEN age &gt;= 10 THEN 'teenager' ELSE 'not adult, not teenager' FROM model WHERE sex = 'female';	last_row_id 얻기user = Model(name='lowell', age=20)session.add(user)session.flush() # DB connection 일어남id = user.id # auto_encrement로 생성된 idsession.commit()검색 (LIKE)results = session.query(Model).\	filter(Model.name.like('김%')).all() # 성이 김씨인 사람 찾음    # 응용해보긔 keyword = kwargs.get('keyword', '')search = True if keyword:    search = Model.name.like(f'{keyword}%')    results = session.query(Model).\	filter(search).all() # 검색어가 있으면 검색, 없으면 모두 가져옴INsession.query(Model).filter(Model.name.in_(('lowell', 'yejin'))).all() # SELECT * FROM model WHERE name IN ('lowell', 'yejin');NOT INsession.query(Model).filter(~Model.name.in_(('lowell', 'yejin'))).all() # SELECT * FROM model WHERE name NOT IN ('lowell', 'yejin');COMMIT, ROLLBACKsession.commit() # commitsession.rollback() # rollbackDATE 계산하기 (DATE_ADD)today = datetime.datetime.now().strftime('%Y-%m-%d')session.query(Model).filter(today &gt;= func.ADDDATE(Model.created_at, 30)).all()# SELECT * FROM model WHERE NOW() &gt;= DATE_ADD(created_at, INTERVAL 30 DAY);referhttps://stackoverflow.com/questions/9667138/how-to-update-sqlalchemy-row-entry]]></content>
      <categories>
        
          <category> TIL </category>
        
      </categories>
      <tags>
        
          <tag> flask orm </tag>
        
          <tag> flask sqlalchemy orm </tag>
        
          <tag> orm 사용법 </tag>
        
          <tag> flask self join </tag>
        
          <tag> sqlalchemy selfjoin </tag>
        
          <tag> sqlalchemy lastrowid </tag>
        
          <tag> sqlalchemy update </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Jeykll Systax highlight 적용하기]]></title>
      <url>/jeykll/2019/04/15/jeykll-syntax-highlight/</url>
      <content type="text"><![CDATA[Jeykll 블로그에서 코드를 올릴 때 syntax highlighting을 해준다면 독자가 글을 읽을 때 직관적이고 편하게 읽을 수 있을 것이다. syntax highlighting을 적용하는 것은  rouge를 통해 쉽게 할 수 있다.1.  gem을 통한 rouge 설치$ gem install kramdown rouge코드 하이라이팅을 위한 rouge를 설치해준다. kramdown은 지킬 마크다운 엔진이다.github.io는 기본으로 kramdown 엔진을 사용한다.2. _config.yml 설정# _config.yml...생략markdown: kramdownhighlighter: rouge3.  테마 다운$ rougify help style...available themes:  base16, base16.dark, base16.light, base16.monokai, base16.monokai.dark, base16.monokai.light, base16.solarized, base16.solarized.dark, base16.solarized.light, colorful, github, gruvbox, gruvbox.dark, gruvbox.light, igorpro, molokai, monokai, monokai.sublime, pastie, thankful_eyes, tulip위 명령어로 지원하는 테마를 확인 한 후 맘에 드는 테마를 다운받으면 된다.$ rougify style monokai &gt; /assets/css/syntax.css4. 적용Html head 부분에 다운 받은 css 파일을 적용해준다....  &lt;link rel="stylesheet" href="/assets/css/syntax.css""&gt;href 안의 링크는 다운 받은 css 파일의 경로를 잘 설정해줘야한다.그럼 끗 ! 💁‍♀️]]></content>
      <categories>
        
          <category> Jeykll </category>
        
      </categories>
      <tags>
        
          <tag> jeykll highlighting </tag>
        
          <tag> 지킬 하이라이트 </tag>
        
          <tag> jeykll syntax highlighting </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Ajax  요청 후 대기 시 액션 처리 하기]]></title>
      <url>/til/2019/04/12/TIL-ajax-loading-action/</url>
      <content type="text"><![CDATA[TIL 카테고리의 글은 그날 배운 것을 정리하는 목적으로 포스팅합니다. 내용이 잘못되었다면 댓글로 피드백 부탁드립니다.상황ajax 요청 후 데이터를 불러 올 때 까지 로딩 바를 띄우고 데이터를 불러왔을 때에는 로딩 바가 사라지게 하고 싶었다.해결$(document).ajaxStart(function() {        // show loader on start         // ajax 요청 후 데이터를 불러 올 때 까지 액션을 취한다.        $("#loader").css("display","block");    }).ajaxSuccess(function() {        // hide loader on success        // ajax 요청 데이터를 성공적으로 받아왔을 때 액션을 취한다.        $("#loader").css("display","none");    });refer  https://stackoverflow.com/questions/20095002/how-to-show-progress-bar-while-loading-using-ajax]]></content>
      <categories>
        
          <category> TIL </category>
        
      </categories>
      <tags>
        
          <tag> ajax request loding </tag>
        
          <tag> ajax loding </tag>
        
          <tag> ajax loading action </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[MySQL ORDER BY 정렬 시 조건 걸기, CASE 사용하기]]></title>
      <url>/til/2019/03/29/TIL-sql-order-by-condition/</url>
      <content type="text"><![CDATA[TIL 카테고리의 글은 그날 배운 것을 정리하는 목적으로 포스팅합니다. 내용이 잘못되었다면 댓글로 피드백 부탁드립니다.쿼리를 통해 데이터를 가져올 때 데이터를 기반으로 조건을 걸어서 정렬하고 싶을 때가 있다. 그때 CASE나 FIELD를 사용하면 가능 조건에 따라 데이터의 정렬 우선순위를 정해줄 수 있다.FIELDORDER BY FIELD (column, 1순위, 2순위, 3순위, n순위...)어떤 데이터의 값의 정렬을 정해주고 싶을 때 사용하면 된다.&gt; SELECT id, title, status FROM movieid title status 1	아바타 	2 2	써니 		 03	극한직업	14	캡틴마블 	1&gt; SELECT * FROM movie ORDER BY FIELD(status, 1, 2, 0) id title status3	극한직업	14	캡틴마블 	11	아바타 	2 2	써니 		 0위 예제는 status의 값 별로 정렬 순위를 정해준 것이다. 값 1이 1순위고 2가 2순위, 0이 3순위인 것을 확인할 수 있다.CASECASE    WHEN &lt;condition1&gt; THEN &lt;result1&gt;    WHEN &lt;condition2&gt; THEN &lt;result2&gt;    WHEN &lt;conditionN&gt; THEN &lt;resultN&gt;    ELSE &lt;result&gt;END;if문과 비슷하게 동작하는 CASE문은 condition에 조건을 넣으면 되고 then 뒤에는 정렬 순위를 넣으면 된다.&gt; SELECT id, title, start_at, end_at  CASE  WHEN start_at &lt;= NOW() AND NOW() &lt; end_at THEN 1    ELSE 2  END movie_order  FROM movie  ORDER BY movie_order위 예제는 오늘이 영화 상영 기간에 포함되어있다면 1순위, 아니면 2순위로 정렬된다.]]></content>
      <categories>
        
          <category> TIL </category>
        
      </categories>
      <tags>
        
          <tag> order by case </tag>
        
          <tag> order by field </tag>
        
          <tag> order by 정렬 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[local storage, session storage]]></title>
      <url>/til/2019/03/21/TIL-local-storage/</url>
      <content type="text"><![CDATA[TIL 카테고리의 글은 그날 배운 것을 정리하는 목적으로 포스팅합니다. 내용이 잘못되었다면 댓글로 피드백 부탁드립니다.상황게시판을 구현하는 중 새로 올라온 게시글을 읽지 않았으면 빨간 닷 표시를 해주고 읽었다면 닷 표시를 없애는 읽음 처리 작업을 해야했다. 카카오톡에서 유저가 프로필을 변경하면 프로필 사진 옆에 빨간 닷 표시가 생기는 것 처럼? 만들고 싶었는데 이를 서버에서 처리하기에는 계속 읽었는지 안읽었는지 확인해야해서 클라이언트 상에서 처리할 수 있는 방안을 찾던 중 회사 프론트 개발자분께서 웹 스토리지를 알려주셨다.스토리지스토리지는 key-value 쌍으로 클라이언트 데이터를 저장할 수 있는 공간이다. 이 스토리지는 두 종류로 되어있는데 로컬 스토리지와 세션 스토리지로 분류되어있다. 각 스토리지는 차이점이 있어서 용도에 따라 알맞게 사용하면 된다.장점  용량이 크다.  HTTP요청 때 전달되지 않아 자원낭비 적음.스토리지는 환경을 많이 타기 때문에 제대로 작동하지 않을 가능성이 있으므로 엄청 중요하진 않지만 있으면 좋은 기능을 구현할 때 사용하면 좋다.아래와 같은 경우 등에서 사용한다고 한다.  팝업  글 작성 시 임시저장용로컬 스토리지로컬 스토리지에 데이터를 저장하면 브라우저를 껐다 키거나 컴퓨터를 껐다 켜도 직접 지우지 않는 이상 절대 지워지지 않는다. 데이터가 영구성을 띈다. 내 경우도 한번 읽은 글에 표시를 할 필요가 없으므로 로컬 스토리지에 데이터를 저장해서 이 데이터로 읽음    처리를 했다. 로컬 스토리지에 쌓인 데이터는 개발자도구 - application - localstorage 에서 확인할 수 있다.세션 스토리지세션 스토리지에 데이터를 저장하면 로컬 스토리지와 반대되게 브라우저, 컴퓨터를 껐다 키면 데이터가 날라간다. 세션 스토리지에는 일회성을 띄는 데이터를 저장하면 된다. 세션 스토리지에 쌓인 데이터는 개발자 도구 - applicaation - sessionstorage에서 확인할 수 있다.Javascript 에서 사용하기로컬 스토리지// set datalocalStorage.setItem('lowell', 'value');// get data localStorage.getItem('lowell'); //vallue// JSON 데이터도 사용가능하다.var data = {'lowell': 20, 'yejin': 23};localStorage.setItem('people', JSON.stringify(data));people = JSON.parse(localStorage.getItem('people'));// remove datalocalStorage.removeItem();세션 스토리지로컬 스토리지 사용법에서 localStroage를 sessionStorage로 변경하면 된다. 메소드는 똑같다!// set datasessionStorage.setItem('lowell', 'value') // get datasessionStorage.getItem('lowell')Refer      https://isme2n.github.io/devlog/2017/06/21/storage-cookie/        https://www.zerocho.com/category/HTML&amp;DOM/post/5918515b1ed39f00182d3048  ]]></content>
      <categories>
        
          <category> TIL </category>
        
      </categories>
      <tags>
        
          <tag> local storage </tag>
        
          <tag> session storage </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[json ValueError Expecting property name.. error]]></title>
      <url>/python/2019/02/28/json-value-error/</url>
      <content type="text"><![CDATA[python에서 dictionary 데이터를 json 형태로 dumps하거나 반대로 json 데이터를 dictionary 형태로 loads할 때 발생하는ValueError: Expecting property name: line 1 column 2 (char 1)Expecting property name enclosed in double quotes에러는 dumps, loads 하려는 데이터가 double quote(쌍 따옴표) 로 감싸지지 않고 single quote(따옴표)로 감싸져 있을 경우 발생한다."{"property": "1"}" # 허용 O"{'property': '1'}" # 허용 X만약에 single quote로 된 데이터를 변경하지 못한다면 모듈 ast를 사용해서 해결 할 수 있다.import astimport jsondata = "{'property': '1'}"# dumps json.dumps(ast.literal_eval(data))# loadsast.literal_eval(data) # return type: dictreferhttps://code.i-harness.com/en/q/1884426https://stackoverflow.com/questions/25707558/json-valueerror-expecting-property-name-line-1-column-2-char-1]]></content>
      <categories>
        
          <category> python </category>
        
      </categories>
      <tags>
        
          <tag> ValueError Expecting property name line 1 column 2 (char 1) </tag>
        
          <tag> Expecting property name enclosed in double quotes </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[일일 커밋 1년 회고]]></title>
      <url>/etc/2019/02/24/%EC%9D%BC%EC%9D%BC%EC%BB%A4%EB%B0%8B-%ED%9A%8C%EA%B3%A0/</url>
      <content type="text"><![CDATA[일일커밋을 시작한지 1년이 되었다. 나는 1년을 기념해서 일일 커밋을 회고하려고 한다.일일커밋을 시작한 이유나는 공부를 열심히 하고자 하는 사람이다. 하지만 게으르고 무기력할 때가 많다. 그래서 하고싶은 공부가 있더라도 실행을 하지 못하는 큰 단점이 있었다. 공부를 할때는 열심히 했지만 아닐 때는 몇일 내내 하지 않는 경우가 많았다. 한번 안하게 되면 계속 안해지고 .. 죄책감도 컸다. 나는 이 문제점에 대해 잘 알고 있었고 해결하려고 노력했으나 고치기엔 쉽지 않았다. 그래서 이 문제를 해결하기 위한 큰 방안이 필요했다.일일 커밋이라는 것은 원래부터 알고 있었다. 취업과 공부 습관에 정말 좋은 공부법이라고 들었던 것 같다. 하지만 처음 들었을 당시에는 ‘일일 커밋은 하루마다 프로그램 코드를 작성하는 것’ 이라는 생각이 컸다. 그래서 불가능해보였다. 하루에 한번 프로그램 코드를 작성하는 것은 그때 당시 나에겐 어려웠던 것이였다. 프로그램을 작성하기 위해선 어떤 프로그램을 만들 것이냐부터 생각을 해야하기 때문이다. 그래서 일일 커밋은 나에게 너무 부담으로 느껴졌다.그러다 공부 습관을 기르는 방법을 찾아보던 중 다시 일일 커밋을 접하게 되었는데, 그때 일일 커밋에 대한 오해가 풀렸다. 꼭 프로그램 코드를 작성해야되는 것이 아니고 다른 공부를 했으면 그 공부한 것에 대한 커밋을 해도 된다는 것을 알게되었기 때문이다. 그리고 TIL이라는 Today I Learned 라는 것을 알게 되었다. 이것은 오늘 내가 공부한 것을 마크다운 문법으로 정리해서 올리는 것이다. 나는 이 TIL이 무척 맘에 들었고 이것을 통해서라면 일일 커밋을 실천할 수 있겠다 생각해서 일일 커밋을 시작하게 되었다.  TIL일일 커밋의 장점1년동안 일일 커밋을 실천해오면서 느낀 장점은 총 3 가지이다  공부 습관을 기를 수 있다.  1년동안 지식이 알차게 쌓인다.  잔디를 심을 수 있다.먼저 하루마다 꼭 커밋을 해야하니 공부를 하게된다. 공부를 해야 커밋을 할 수 있으니까. 솔직히 말하면 공부를 안해도 커밋한 적이 적지 않다. 놀거나 아프거나 집에 일이 있거나.. 그럴 때에는 전에 공부했던 것을 정리하거나 진짜 대충 어디서 주워들은 용어의 정의를 정리해서 올렸다. 그래도 이렇게 매일 공부를 하고.. 커밋을 하면서 이제는 하루라도 공부를 안하면 뭔가 불안한 느낌이 든다. 그리고 그게 당연시 된 것 같다. 예전처럼 몇 일 열심히 공부하고 몇 일 엄청 늘어지는 그런 악순환이 없어졌다. 꾸준하게 공부하는 습관을 기른 것 같다. 습관 기르는게 정말 힘든데 이 부분은 일일 커밋이 정말 큰 도움이 되었다.그리고 당연한 얘기지만 1년동안 열심히 공부를 하게되니 지식이 쌓인다. 지난 2018년을 돌아보면 정말 많은 것을 공부했다는 생각이 든다. 현재 내가 아는 지식의 대부분은 지난 1년동안 공부했던 것들 같다. 내가 공부를 시작한지 오래된 건 아니지만.. 일일커밋을 시작하기 전과 시작한 후의 습득량은 확연히 차이난다.마지막으로는 깃허브의 잔디를 빼곡히 심을 수 있다는 점이다. 잔디가 채워지는 것은 정말 보기 좋다. 그리고 요즘은 깃허브도 포트폴리오로 속하기 때문에 관리를 잘 해줘야하는데 열심히 관리해줘야하는 부분이 잔디라고 생각한다. 듬성 듬성난 잔디보다는 빼곡하게 채워진 잔디밭이 더 보기도 좋기 때문이다.  나는 1년동안 3일 커밋을 하지 못했다. 아팠을 때, 까먹었을 때, 졸업식 날.. 그때마다 하루씩 비어지는 잔디때문에 마음이 너무 아팠다. 공부를 못했다는 생각보다 풀잔디가 아니라는… 흑 그게 너무 슬펐다. ㅜ_ㅜ일일커밋의 단점내가 생각하는 일일 커밋의 단점은 짧고 굵게 말하자면 스트레스이다.매일 매일 커밋을 해야한다는 그런 생각과 부담감과 책임감이 스트레스를 가져왔다. 회사나 학교를 갔다온 후 엄청 피곤한 날에도  ‘그래도 공부해야지’ 가 아닌 ‘그래도 커밋해야지’라는 생각이 든다는 단점.  그 부분이 나에겐 스트레스가 된 것 같다. 그래도 이런 강제성 때문에 공부를 하게 되었고 습관도 늘게되었지만..그리고 커밋을 하기 애매한 공부를 했을 때. 예를 들어 하루종일 공부하기는 했으나 공부할 프로그램을 설치하느라 시간을 다 소모했을 때. 이럴 때는 공부는 했는데 커밋할 내용이 없어 난감하다. 설치 방법을 쓸 수 도 있지만 그 것을 쓰기에는 시간이 부족할 때.. 이럴 때 애매하다.또한 블로그 글을 쓸 때에도 방해가 되었다. 블로그 글을 쓸 때 최소 2-3시간 정도 소요되는데 그러면 커밋을 못할 수도 있기 때문에 글을 쓰는 것을 멈추고 TIL 커밋을 했었다. 이것이 내가 티스토리에서 운영하던 블로그를 github.io로 이전한 이유이다.일일 커밋을 하면서 약간 신데렐라가 된 느낌이었다. ‘12시 안에 얼른 커밋해야돼!!!!’ 이런 생각을 하고 지내왔다. 내가 잘 못된 부분이였을 수도 있지만.. 11시부터 공부를 시작하는데 12시 안에 커밋을 하지 못할 거라고 생각이 되면 공부하고 싶은 분야는 그냥 안하고 빨리 공부할 수 있는 분야를 공부하기도 했다.결론나는 예전의 나와 같은 고민을 하고 있는 사람이라면 꼭 일일 커밋을 해봤으면 좋겠다고 생각한다. 깃허브를 포트폴리오 역할로 사용하고 싶은 사람도 마찬가지이다. 일일 커밋은 정말 도움이 되는 공부 방법이자 포트폴리오라고 생각한다. 나도 정말 많은 도움이 되었고 1년동안 성장할 수 있었던 큰 요소는 이 일일 커밋이라고 생각한다.하지만 나는 오늘 이후로 일일커밋 실천을 중단하려고 한다. 단점 부분에서 말했듯이 일일 커밋을 하며 스트레스를 알게 모르게 많이 받아왔던 것 같다. ‘이 ‘공부를 하고싶은에.. 커밋을 해야하니까 일단 빨리 정리할 수 있는 공부를 하고 다음에 하자’, ‘아 이거 설치하면 시간 다가는데 언제 커밋하지’ 이런 것들이 나를 피곤하게 만들었다.그래서 이제는 내가 하고 싶은 공부를 계속해서 하고, 더 다양한 분야를 공부하기 위해 그만하려고 한다. 아예 깃허브 활동을 하지 않는 다는 것은 절대 아니다. 매일 매일 커밋해야한다는 강제성을 띄는 생각을 그만 한다는 것이다.나는 1년동안 일일 커밋을 통해 기른 소중한 공부 습관을 가지고서 더욱 더 성장하고 싶고 그럴 거라는 확신이 있다. 그만두더라도 나의 잔디밭은 항상 무성할 것이다!]]></content>
      <categories>
        
          <category> ETC </category>
        
      </categories>
      <tags>
        
          <tag> 일일커밋 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[데이터 분석이란]]></title>
      <url>/til/2019/02/12/TIL-data-analysis/</url>
      <content type="text"><![CDATA[TIL 카테고리의 글은 그날 배운 것을 정리하는 목적으로 포스팅합니다. 내용이 잘못되었다면 댓글로 피드백 부탁드립니다.데이터 분석  어떤 입력 데이터가 주어졌을 때 입력 데이터 간의 관계를 파악하거나 파악된 관계를 사용하여 원하는 출력 데이터를 만들어 내는 과정입력 데이터 input data  X  분석의 기반이 되는 데이터  독립변수(independent variable), 특징 (feature), 설명변수출력 데이터 output data  추정하거나 예측하고자 하는 목적 데이터  Y  종속변수 (dependent variable)예측 하기위한 방법론  규칙기반 - 알고리즘  학습기반, 데이터 기반 - 대량의 데이터를 컴퓨터에 보여줌으로써 스스로 규칙을 만들게 하는 방법 스스로 규칙을 찾도록 함. (머신러닝)지도학습학습용 데이터 집합 training data set입력값과 목푯값을 쌍으로 가지는 샘플 데이터의 집합레이블링 : 입력 데이터에 사람이 원하는 목표값을 붙여주는 것 -&gt; 데이터 양이 많을 때는 상당히 어려운 일이 될 수 있음.전처리와 인코딩문서나 이미지와 같은 현실의 데이터를 컴퓨터가 처리할 수 있는 숫자데이털 바꾸는것문서 데이터 인코딩 : 문서 데이터를 고정된 크기의 숫자 벡터로 바꿈BOW Bag of Words문서를 이루는 단어의 순서, 의미 등의 정보를 무시하고 특정한 단어가 문서에 몇번 나왔는지 세어 빈도를 벡터로 표시하는 방법카테고리 값숫자 외 컴퓨터가 다룰 수 있는 데이터범주형 값숫자와 달리 기호로 표시되며 비연속적. 두개의 데이터가 있을 때 이들의 크기나 가치 순서를 비교할 수 없음.클래스 : 카테고리 값이 가질 수 있는 경우의 수동전을 던진 결과와 같은 2가지 경우의 수 - 이진 클래스주사위를 던져 나온 경우의 수 - 다중 클래스숫자처럼 비교가능한 경우도 있음.회귀분석출력하고자 하는 값이 숫자인 경우 : 회귀분석  , 카테고리 값인 경우 : 분류분류 : 4지 선다형 객관식 문제회귀 분석 : 직접 답을 써야하는 단답형 문제비지도 학습입력 출력이 구분되지 않는 단순한 데이터들의 관계에서 특정한 규칙을 찾아내는 것입력, 출력 데이터를 구분짓지 않고 단순히 데이터를 입력하면 이 데이터들간의 규칙을 찾아내거나 미리 지정한 규칙에 맞는 데이터인지를 구분함.클러스터링대표적인 비지도 학습 방법 중 하나.데이터들을 유사한 데이터까지 같은 그룹으로 모으는 클러스터링 방법]]></content>
      <categories>
        
          <category> TIL </category>
        
      </categories>
      <tags>
        
          <tag> 데이터 분석이란 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[mysql count()의 default를 0으로 되게 하기]]></title>
      <url>/mysql/2019/02/01/mysql-count-default-0/</url>
      <content type="text"><![CDATA[count() 함수를 쓴 쿼리 조회시 해당되는 결과가 없을 시 0이 나오게 하고 싶을 경우가 있다.그럴때에는 LEFT JOIN 을 사용하면 원하는 값을 얻을 수 있다.SELECT s.id, count(hi.id) as 'history_num'FROM student s LEFT JOIN history hi ON s.id = hi.sidWHERE s.id IN (1, 2, 3) GROUP BY u.id;# 결과s.id   history_num1	   02 	   103      22추가로 ` LEFT JOIN`할 테이블에 조건을 추가 하고 싶을 경우 서브쿼리를 사용하면 된다.]]></content>
      <categories>
        
          <category> mysql </category>
        
      </categories>
      <tags>
        
          <tag> mysql count default 0 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[mysql upsert]]></title>
      <url>/db/2019/01/21/TIL-db-upsert/</url>
      <content type="text"><![CDATA[TIL 카테고리의 글은 그날 배운 것을 정리하는 목적으로 포스팅합니다. 내용이 잘못되었다면 댓글로 피드백 부탁드립니다.upsert데이터베이스에서 만약 특정 값이 없다면 INSERT를, 있다면 update를 해야하는 경우가 있을 것이다. 나는 원래직접 특정 값을 select해서 가져온 값이 있다면 update 없다면 insert 를 했었는데 mysql에서는 UPSERT를 지원한다.INSERT INTO table (id, comment) VALUES (1, 'hello')ON DUPLICATE KEY UPDATE # update     comment = 'hello'위 쿼리에서 id 는 테이블에 해당 값이 있는지 확인하는 컬럼이다. 이 id가 있다면 UPDATE를 수행하고 없다면 INSERT를 수행하게 된다.]]></content>
      <categories>
        
          <category> db </category>
        
      </categories>
      <tags>
        
          <tag> upsert </tag>
        
          <tag> db upsert </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[AWS Route 53]]></title>
      <url>/til/2019/01/20/TIL-aws-route53/</url>
      <content type="text"><![CDATA[TIL 카테고리의 글은 그날 배운 것을 정리하는 목적으로 포스팅합니다. 내용이 잘못되었다면 댓글로 피드백 부탁드립니다.Route 53  높은 가용성과 확장성이 뛰어난 클라우드 Domain Name System (DNS)웹 서비스DNS를 서비스화 시킨 것.  도메인 네임 서버를 임대해줌. (도메인 네임서버 장만)  AWS 컴퓨팅 서비스와 연동이 가능함.  도메인 등록을 할 수 있음.프로세스  도메인 구입  산 도메인에 서버 IP 연결  NS 설정Route 53은 도메인 구매대행, 등록 대행자라고 생각하면 됨.refer생활코딩 Route 53]]></content>
      <categories>
        
          <category> TIL </category>
        
      </categories>
      <tags>
        
          <tag> router53 </tag>
        
          <tag> aws router53 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[mysql-python 설치 시 에러 해결]]></title>
      <url>/python/2019/01/18/mysql-python-error-solve/</url>
      <content type="text"><![CDATA[mysql-python 설치 시 command 'clang' failed with exit status 1 이런 오류를 볼 수 있는데 아래와 같이 해결할 수 있다.# brew install mysql brew unlink mysql# brew install mysql-connector-c brew link mysql-connector-c# 설치pip install mysqlclient pip install mysql-pythonbrew unlink mysql-connector-cbrew link mysql나는 이 방법으로 configparser 에러 등 mysql 파이썬 드라이버를 설치할 때 나는 오류를 해결 할 수 있었다!referhttps://stackoverflow.com/questions/50940302/installing-mysql-python-causes-command-clang-failed-with-exit-status-1-on-mac]]></content>
      <categories>
        
          <category> python </category>
        
      </categories>
      <tags>
        
          <tag> command 'clang' failed with exit status 1 </tag>
        
          <tag> mysql-python 설치 에러 </tag>
        
          <tag> mysqlclient 설치 에러 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[db index]]></title>
      <url>/til/2019/01/17/TIL-index/</url>
      <content type="text"><![CDATA[TIL 카테고리의 글은 그날 배운 것을 정리하는 목적으로 포스팅합니다. 내용이 잘못되었다면 댓글로 피드백 부탁드립니다.Index  지정한 컬럼들을 기준으로 메모리 영역에 목차를 생성하는 것  SELECT 의 성능을 향상되지만 INSERT,UPDATE,DELETE 성능은 떨어지게됨.  디스크에서 읽는 것은 메모리에서 읽는 것보다 성능이 떨어짐.  인덱스의 성능을 향상 시킨다는 것은 디스크 저장소에 얼마나 덜 접근하느냐, 인덱스 roof에서 leaf 까지 오고 가는 횟수를 얼마나 줄이느냐임.  인덱스 개수는 3~4개가 적당함.인덱스 칼럼 기준1개의 컬럼만 인덱스를 걸어야 한다면?카디널리티(cardinaliry)가 가장 높은 것을 잡아야함.카디널리티란 해당 컬럼의 중복된 수치를 뜻함. 데이터가 중복될 가능성이 많은 성별, 학년 등은 카디널리티가 낮다고 하고 유니크한 데이터를 가지는 주민등록번호, 계좌번호 등은 카디널리티가 높다고 함.(pk 성질이 높은 것)인덱스를 최대한 효율적으로 뽑을려면 해당 인덱스로 많은 부분을 걸러야하기 때문임. 성별이라면 남/녀 중 하나기때문에 인덱스를 통해 50% 밖에 걸러내지 못함.여러 컬럼으로 인덱스 구성하려면?카디널리티가 높은순 -&gt; 낮은순으로 인덱스를 걸면 좋음.여러 컬럼 인덱스 타기조회 쿼리 사용시 인덱스를 태우려면 최소한 첫번째 인덱스 조건은 조회 조건에 포함되어있어야 함. 첫번째 인덱스 컬럼이 조회 쿼리에 없으면 인덱스를 타지 않기 때문.주의사항  범위 조건(&gt;, BEETWEEN)은 해당 컬럼은 인덱스를 타지만 그 뒤 인덱스 칼럼들은 인덱스 사용이 안됨.      WHERE에서 or 사용은 주의해야함. (비교해야할 row가 늘어나기 때문)    인덱스로 사용된 컬럼 값 그대로 사용해야 인덱스를 탐. (가공된 데이터를 저장하지 않음)  조회 조건 순서는 중요하지 않음.]]></content>
      <categories>
        
          <category> TIL </category>
        
      </categories>
      <tags>
        
          <tag> db 인덱스 </tag>
        
          <tag> database index </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[data warehouse]]></title>
      <url>/til/2019/01/17/TIL-data-warehouse/</url>
      <content type="text"><![CDATA[TIL 카테고리의 글은 그날 배운 것을 정리하는 목적으로 포스팅합니다. 내용이 잘못되었다면 댓글로 피드백 부탁드립니다.Data Warehouse  사용자의 의사 결정에 도움을 주기 위해 데이터베이스에 축적된 데이터를 공통의 형식으로 변환해서 관리하는 데이터베이스  데이터를 저장하는 창고  모든 데이터를 저장하기 위한 시스템  데이터 구조 설계 및 분석 방법까지도 포함해 데이터 기반 의사 결정을 할 수 있도록 지원하는 시스템.시스템에서 이뤄지는 작업ETL(Extraction, Transformation, Loading)수집 단계, 수집된 데이터를 report/dashboard 등으로 정리하는 리포팅 단계, 분석 단계의 데이터를 모은 장소가 데이터 웨어하우스이다.AWS Redshift  클라우드에서 실행되는 신속하고 강력한 페타바이트 규모의 SQL 기반 데이터 웨어하우스 서비스referhttps://medium.com/team-mangoplate/%EB%A7%9D%EA%B3%A0%ED%94%8C%EB%A0%88%EC%9D%B4%ED%8A%B8-%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%9B%A8%EC%96%B4%ED%95%98%EC%9A%B0%EC%8A%A4-%EC%9D%B4%EC%95%BC%EA%B8%B0-602e4e073078####]]></content>
      <categories>
        
          <category> TIL </category>
        
      </categories>
      <tags>
        
          <tag> DW </tag>
        
          <tag> data warehouse </tag>
        
          <tag> 데이터 웨어하우스 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[AWS CloudFront]]></title>
      <url>/til/2019/01/11/TIL-aws-cloudfront/</url>
      <content type="text"><![CDATA[TIL 카테고리의 글은 그날 배운 것을 정리하는 목적으로 포스팅합니다. 내용이 잘못되었다면 댓글로 피드백 부탁드립니다.CloudFront  빠르고 고도로 안전하며 프로그래밍 가능한 콘텐츠 전송 네트워크(CDN)CloudFront는 캐시를 제공하는 캐시서버이면서 CDN임.  캐시 서버 역할 제공  고속으로 컨텐츠 전송 (CDN)CDN (Content Delivery Network) 전 세계 어디있더라도 컨텐츠를 빠르게 제공할 수 있음.ColudFront의 캐시 서버인 edge location이 전 세계 다양한 곳에 서비스 제공되어 있어 가능함.사용법은 사용자가 클라우드 프론트(distribution)을 생성하고  자신의 서버(origin)를 등록한다. 이후 클라우드 프론트의 주소를 실제 서버 도메인 대신 사용하면 빠르게 접속할 수 있을 것이다.캐시 설정  캐시 유효시간 설정 Object caching -&gt; customize -&gt; TTL 시간을 정해준다.          Minimum TTL : 캐시 최소 유효시간      Maximun TTL : 캐시 최대 유효시간      CloudFront의 기본 캐시 유효시간은 24시간        querystring 설정          Query string forwarding and caching을 forward all로 설정해야 querystring이 먹음.        price class 설정          price class를 통해 접속 가능한 지역을 선택할 수  있음      미국, 캐나다 유럽 -&gt; 저렴, 호주 아프리카 -&gt; 비싸짐      기타  Router 53 서비스를 통해 클라우드 프론트의 주소를 아름답게 할 수 있음  다양한 보안 (AWS ACM)]]></content>
      <categories>
        
          <category> TIL </category>
        
      </categories>
      <tags>
        
          <tag> AWS cloudfront </tag>
        
          <tag> cloudfront </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[serverless]]></title>
      <url>/til/2019/01/08/TIL-serverless/</url>
      <content type="text"><![CDATA[TIL 카테고리의 글은 그날 배운 것을 정리하는 목적으로 포스팅합니다. 내용이 잘못되었다면 댓글로 피드백 부탁드립니다.서버리스란 서버가 없다. 라는 뜻이다. 진짜 서버가 없는 것은 아니고 특정 작업을 수행하기 위해 컴퓨터나 가상머신에서 서버를 설정해 이를 처리하는 것이 아닌 것을 뜻한다.서버리스를 알아보기 전에 먼저 기존에 있던 기술을 알아보자.IaaS (Infrastructure as a Service)  AWS, AzureIaaS를 사용하면 서버 자원, 네트워크 전력 등의 인프라를 모두 구축할 필요가 없다. 단 클릭 몇 번 만으로도 나만의 서버가 생성된다.  이로인해 어플리케이션을 직접 사용자에게 서비스하는 것이 매우 쉬워지고 편리해졌다.PaaS (Platform as a Service)IaaS에서 한번 더 추상화 된 모델이다. 네트워크, 런타임까지 제공되며 어플리케이션만 배포하면 바로 구동이 가능하다.Serverless서버의 존재에 대해 신경을 쓰지 않아도 된다. 어떤 사양으로 돌아가는지, 서버의 갯수를 늘려야 하는지, 네트워크는 어떤 걸 사용할지 설정하지 않아도 된다.BaaS (Backend as a Service)  Firebase앱 개발에 있어 다양한 기능(데이터베이스, 소셜서비스 연동, 파일시스템)을 API로 제공해줌으로써 개발자들이 서버 개발을 하지 않고 필요한 기능을 쉽고 빠르게 구현할 수 있도록 해준다. 비용은 사용한 만큼만 나가게 된다. 또한 서버 확장도 알아서 해주게 된다.단점은 백앤드 로직이 클라이언트 쪽에 구현이 되기 때문에 클라이언트 코드가 무거워 질 수 도 있다는 점. 복잡한 쿼리가 불가능해진다.FaaS (Function  as a Service)  AWS lambda프로젝트를 여러 개의 함수로, 혹은 한개의 함수로 만들어서 컴퓨팅 자원에 함수를 등록하고 함수를 호출할 수 있다.  함수는 특정 이벤트가 했을 때 실행된다. FaaS는 주기적 처리, 웹 요청 처리, 콘솔 호출이 가능하다. 만약 8시간마다 크롤링하는 함수가 있으면 FaaS를 사용하는 것이 적합하다.PaaS와의 차이점은 PaaS는 전체 애플리케이션을 배포하고 애플리케이션이 24시간 계속해서 돌아간다면 FaaS는 애플리케이션이 아닌 함수를 배포하고 특정 이벤트가 발생했을 때 실행되고 작업을 마치면 종료가 된다.Referhttps://velopert.com/3543]]></content>
      <categories>
        
          <category> TIL </category>
        
      </categories>
      <tags>
        
          <tag> serverless </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Http Cache]]></title>
      <url>/til/2018/12/20/http-cache/</url>
      <content type="text"><![CDATA[TIL 카테고리의 글은 그날 배운 것을 정리하는 목적으로 포스팅합니다. 내용이 잘못되었다면 댓글로 피드백 부탁드립니다.Cache  데이터의 전송 속도를 높이기 위해서 고안된 것이미 다운로드 받은 파일을 왜 다시 받아야하지?  다운로드 받은 파일은 캐시(저장)했다가 같은 파일을 요청해 그 파일을 열게 되면 훨씬 빠르게 접근할 수 있음.cache 설정no-store : 캐시 금지max-age : 캐시 만기 시간  max-age=300 이라면 5분마다 캐시가 갱신됨.no-cache : 캐시가 유효한지 확인  response header에 보면 last modified가 있는데 이것은 서버에 위치한 파일이 마지막으로 수정된 시간을 뜻함. 캐시가 만료될 시 클라이언트와 서버의 파일의 last modified 값을 비교해 시간이 같다면 파일이 수정되지 않았으므로 응답 헤더만 주고 받음.  no-cache는 max-age=0과 같음.ETag : last modified보다 정확한 파일 수정 비교 값  웹 서버는 ETag와 last modified를 비교해 둘 중 하나만 달라도 다시 파일을 응답함.refer생활코딩 http cache]]></content>
      <categories>
        
          <category> TIL </category>
        
      </categories>
      <tags>
        
          <tag> http cache </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[AWS S3]]></title>
      <url>/til/2018/12/18/TIL-aws-s3/</url>
      <content type="text"><![CDATA[TIL 카테고리의 글은 그날 배운 것을 정리하는 목적으로 포스팅합니다. 내용이 잘못되었다면 댓글로 피드백 부탁드립니다.S3  Simple Storage Service간단하게 말해서 파일을 저장할 수 있는 파일 서버.심플 스토리지라고 되어있지만 심플은 아님 😅장점  내구성이 높음          파일을 저장하면 한 곳에 저장되는 것이 아닌 여러 곳(각 다른 지역)에서 저장됨.      삭제 염려가 없다.        인터넷에 연결되어 있어 파일에 접속해서 다운로드가 가능함. (파일 서버)  버전관리 기능 제공  등구조      Bucket    S3 프로젝트를 말함.        폴더        Object    파일  권한퍼블릭 엑세스 : 버킷을 공개할 것인지, 하지 않을 것인지 권한을 설정함.]]></content>
      <categories>
        
          <category> TIL </category>
        
      </categories>
      <tags>
        
          <tag> AWS S3 </tag>
        
          <tag> S3 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[python pdb로 디버깅하기]]></title>
      <url>/python/2018/12/11/python-debuging-pdb/</url>
      <content type="text"><![CDATA[오늘 소개할 것은 파이썬 디버깅 도구 pdb이다.나는 IDE를 통한 디버깅은 뭔가 복잡해서 맘에 안들었는데 pdb는 간단하고 가벼워서 정말 좋당.pdb는 파이썬에서 제공하는 기본 모듈이므로 따로 설치가 필요없다.사용법 또한 간단하다.def sum(x, y):	import pdb	pdb.set_trace() # 디버깅을 시작하는 구간 (앤드포인트)		result = x + y	return resultif __name__ = '__main__':    print(sum(3,7))위 코드와 같이 디버깅을 하고 싶은 부분에 pdb를 임포트하고 pdb.set_trace()메서드를 적어준다.그 다음 코드를 실행을 하면 pdb.set_trace()를 작성한 부분 다음부터 디버깅을 할 수 있다.디버깅은 터미널을 통해서 할 수 있다.-&gt; result = x+y(Pdb) x3(Pdb) type(y)int(Pdb) n&gt; /Users/yejin/dev/sum.py(6)sum()-&gt; return result(Pdb) result10실행하면 앤드포인트에서 실행이 중지되고 n을 누르면 한줄 한줄 실행되게 된다.또한 변수명을 입력하게 되면 변수명에 어떤 값이 들어있는지도 확인이 가능하고 변수의 값을 가공해서 어떤 값이 나오는지 확인도 가능하다.이 pdb의 단점은 함수 안으로 들어가지 못한다는 것이다. 그렇기 때문에 함수 안에 들어가서도 값을 확인해야한다면 set_trace()를 함수 안에다 옮겨 놓아야한다.]]></content>
      <categories>
        
          <category> python </category>
        
      </categories>
      <tags>
        
          <tag> 파이썬 디버깅 </tag>
        
          <tag> python debuging </tag>
        
          <tag> python pdb </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[File system]]></title>
      <url>/til/2018/12/10/TIL-file-system/</url>
      <content type="text"><![CDATA[TIL 카테고리의 글은 그날 배운 것을 정리하는 목적으로 포스팅합니다. 내용이 잘못되었다면 댓글로 피드백 부탁드립니다.file  저장의 최소 단위. 바이트의 연속적인 연결파일은 바이트의 연속적인 연결이기 때문에 구조가 없음.파일의 속성  이름  식별자  유형  위치  크기  보호 (권한)  시간, 날짜 사용자 식별파일 연산  생성          공간 할당, 파일과 관련된 정보를 디렉토리에 추가        쓰기  읽기  파일 내 위치 변경  삭제          내용 삭제, 공간 회수        자르기          내용은 삭제되지만 공간은 회수되지 않음.        열기  닫기open-file-table최초 파일 접근 후 (open)재접근에 대한 낭비를 줄이기 위해 모든 열린 파일에 대한 정보가 저장됨.매번 디렉토리를 검색하는 비용을 줄임.open-file-count파일에 대해 몇개의 프로세스가 접근했는지 나타냄.open file locking  sheard lock 여러 프로세스가 잠금을 할 수 있음.  exclusive lock 한번에 한 프로세스만 잠금을 하는 것파일 접근 방법  sequential access          순차적으로 접근      가장 일반적임        direct access          직접 접근. 파일은 고정 길이의 논리 레코드의 집합으로 정의되어야함.      레코드의 번호로 접근.        기타 접근 방법          파일에 대한 인덱스index를 통해 작동할 데이터의 위치를 신속하게 찾아냄.      디렉토리디렉토리는 파일 시스템에 포함되어 있으며 디렉토리 안에 있는 파일의 정보들을 가지고 있음.디렉토리 구조  Single-Level Directory          한개의 폴더에 한개의 파일만 존재하는 것.      각 파일의 이름은 고유해야한다.      그룹핑도 하지 못함.              Two-Level Directory          마스터 파일 디렉토리 밑에 유저 파일 디렉토리가 있는 구조      각 유저당 동일한 이름의 파일을 가지고 있음      path 개념이 생김.                  Tree-Structure Directories        우리가 아는 트리 구조의 디렉토리        디렉토리 밑에 하위 디렉토리를 생성하는 구조        운영체제는 하위 디렉토리를 하나의 파일로 봄.    ]]></content>
      <categories>
        
          <category> TIL </category>
        
      </categories>
      <tags>
        
          <tag> file system </tag>
        
          <tag> 파일 시스템 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[JAVA this와 this()의 차이점]]></title>
      <url>/java/2018/12/09/java-this-%EC%B0%A8%EC%9D%B4%EC%A0%90/</url>
      <content type="text"><![CDATA[자바에는 this와 this() 키워드가 있는데 이 키워드는 전혀 다른 키워드이다.짧게 설명하자면 this는 인스턴스 자신을 가르키는 참조 변수이고 this()는 생성자를 뜻한다.thisclass Car {    String color; // 인스턴스 변수    String gearType;    int door;         Car(String color, String gearType, int door){        this.color = color;         this.gearType = gearType;        this.door = door;    }}this 는 위 코드처럼 생성자의 매개변수로 선언된 변수의 이름이 인스턴스 변수와 같을 때 인스턴스 변수와 지역변수를 구분하기 위해서 사용한다.Car() 생성자 안에서의 this.color는 인스턴스 변수이고, color는 매개변수로 정의된 지역변수이다.static 메서드에서는 this를 사용하지 못한다.this()class Car{	String color; // 인스턴스 변수    String gearType;    int door;         Car(){        this("white", "auto", 4); // Car(String color, string gearType, int door)를 호출    }        Car(String color){        this(color, "auto", 4);    }        Car(String color, String gearType, int door){        this.color = color;         this.gearType = gearType;        this.door = door;    }}this()는 같은 클래스의 다른 생성자를 호출할 때 사용한다. 위 코드의 Car() 생성자와 Car(String color) 생성자는 this()를 통해  모두 Car(String color, String gearType, int door) 생성자를 호출하고 있는 것이다.refer자바의 정석 1]]></content>
      <categories>
        
          <category> JAVA </category>
        
      </categories>
      <tags>
        
          <tag> this this() </tag>
        
          <tag> this this() 차이점 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[storage system]]></title>
      <url>/til/2018/12/08/TIL-storage-system/</url>
      <content type="text"><![CDATA[TIL 카테고리의 글은 그날 배운 것을 정리하는 목적으로 포스팅합니다. 내용이 잘못되었다면 댓글로 피드백 부탁드립니다.디스크 스케줄링운영체제는 디스크를 효율적으로(탐색 시간 최소화) 사용하기 위해 여러 기법을 사용함.  seek time : 탐색시간. 디스크의 접근시간에 가장 큰 영향을 줌.  디스크의 입출력 요청에 포함되는 정보          연산의 유형 : 입력 or 출력      디스크 주소      메모리 주소      전송할 바이트의 수        디스크 드라이브와 제어기가 여유상태이면 바로 처리되고 아니라면 대기 큐에서 대기함.현재 대기 큐에 98, 183, 37, 122, 65, 67  이렇게 들어와있고 현재 디스크의 헤드는 53을 가르키고 있다고 가정한다.FCFS  대기 큐에서 가장 먼저 요청을 한 것 부터 처리를 함.헤드의 움직임을 최적화  하지 않기 때문에 성능이 나쁠 수 있음.SSTF (Shortest-Seek-Time-First)  seek time이 가장 작은 것 부터 먼저 처리하는 방식  계속해서 탐색 시간이 작은 요청이 큐에 쌓이게 되면 탐색 시간이 긴 요청들은 처리되지 않는 starvation(기아현상)이 발생할 수 있음.SCAN  디스크의 한 쪽 끝에서부터 다른 쪽 끝으로 이동한해 처리한 다음 방향을 바꾸어 처리하는 방식  끝과 끝을 왕복하는 모습이 엘레베이터와 비슷해서 elevator algorithm라고 불림.  비정상적으로 양 끝단의 request가 들어오면 문제가 발생할 수 있음.C-SCAN  끝을 향해 이동하다가 끝에 도달하게 된다면 아예 처음으로 이동 후 다시 시작함.  SCAN의 발전된 알고리즘C-LOOK  C-SCAN과 달리 처음으로 돌아갈 때 맨 처음(0), 맨 끝으로 돌아가는 것이 아닌 가장 작은 request 의 위치와 가장 큰 request 위치로 이동함.  디스크에 부하가 가는 작업은 SCAN이나 C-SCAN이 유리함.  보통 SSTF나 LOOK을 사용함.]]></content>
      <categories>
        
          <category> TIL </category>
        
      </categories>
      <tags>
        
          <tag> storage system </tag>
        
          <tag> 디스크 스케줄링 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Virtual Momery]]></title>
      <url>/til/2018/12/02/TIL-virtual-memory/</url>
      <content type="text"><![CDATA[TIL 카테고리의 글은 그날 배운 것을 정리하는 목적으로 포스팅합니다. 내용이 잘못되었다면 댓글로 피드백 부탁드립니다.가상 메모리  Virtual Memory 한정된 물리 메모리의 한계를 극복하고자 디스크와 같은 저장장치를 활용해 애플리케이션들이 더 많은 메모리를 활용할 수 있게 해주는 것  어떤 프로세스를 실행할 때 프로세스 전체가 메모리에 적재되지 않고도 실행이 가능 하도록 함.      현재 필요한 부분만 메모리에 적재하도록 할 수 있음.    (실제 메모리) physical memory보다 주소공간이 더 클 수 있음.  파일이나 메모리가 둘 또는 그 이상의 프로세스들에 의해 공유되는 것을 가능하게 함.책상에 많은 책을 펴놓고 공부를 할 때 책상에 해당되는 것이 실제 물리 메모리이고 책상에 책을 놔둘 공간이 없어 책을 꽃아놓는 책장이 가상 메모리라고 생각하면 된다.책상에 펴져있는 책들은 실행 중인 프로세스들이고 책장에 꽂아져있는 책들은 프로그램들이다.요구 페이징 (Demand Paging)  초기에 필요한 것만 실제 메모리에 적재하고 페이지들이 실행 과정에서 실제로 필요할 때 적재하는 기법.  한번도 접근되지 않는 페이지는 물리 메모리에 적재되지 않게 됨.  스와핑 기법과 비슷함.  pager : 페이지들의 연속. 프로세스 내의 개별 페이지들을 관리함.  디스크에서 메모리로 페이지를 적재하는 swap in을 수행하는 경우 프로세스가 다시 swap out되기 전에 실제로 어떤 페이지들이 사용될 것인지 추측하고 실제로 필요한 페이지들만 메모리로 읽어옴.프로세스 전체가 아닌 부분 조각 page들만 가져오기 때문에 특정 프로세스 내에서 어떤 페이지가 실제 메모리에 적재 되어있는지 아닌지를 구별해야함.page table의 valid-invalid bit 를 이용해 판단함.page table  페이지 테이블에는 각각의 page에 맵핑되는 frame과 valid-invalid bit로 구성되어 있음.  valid-invalid bit를 통해 어떤 페이지가 실제 물리 메모리에 적재되어있는지 표시해주고 만약 실제 물리 메모리에 적재되어있지 않다면 그 페이지가 저장되어있는 디스크 주소를 기록해 나중에 필요할 때 디스크에서 가져올 수 있게 함.page fault  실제 물리 메모리에 적재되어있지 않는 페이지를 접근하려고 할 때 발생하는 트랩.현재 접근하려고 하는 페이지 테이블의 valid-invalid bit 항목이 무효로 되어있으면 발생함.순수 요구 페이징 (pure demand paging)  어떤 페이지가 필요해지기 전에는 페이지를 적재하지 않는 것순수 요구 페이징을 사용하면 맨 처음부터 page fault가 발생함.page fault handling (페이지 부재 처리)  프로세스에 대한 내부 테이블을 검사해서 그 메모리 참조가 유효(valid)/무효(invalid) 인지를 알아낸다.      만약 무효한 페이지에 대한 참조라면 그 프로세스는 중단된다. 만약 유효한 참조인데 페이지가 아직 메모리에 올라오지 않았다면, 그것을 디스크로부터 가져와야 한다.    빈 공간 즉, 자유 프레임을 찾는다. (예를 들면, 페이지 프레임 리스트에서 하나 가져옴)  디스크에 새로이 할당된 프레임으로 해당 페이지를 읽어 들이도록 요청한다.      ​디스크 읽기가 끝나면, 이 페이지가 이제는 메모리에 있다는 것을 알리기 위해 페이지 테이블을 갱신하며, 프로세스가 유지하고 있는 내부 테이블을 수정한다.    트랩에 의해 중단되었던 명령을 다시 수행한다. ​이제 프로세스는 마치 그 페이지가 항상  메모리에 있었던 것처럼 해당 페이지를 접근할 수 있다.지역성의 원리 (locality of reference)  CPU가 참조하는 주소가 지역에 모여져있다.한번 읽었던 코드(주소)를 다시 읽을 확률(while, for)이 높기 때문에 page fault가 문제가 될 만큼 많이 일어나지 않음.페이지 교체 (Page Replacement)  프로세스가 메모리보다 더 큰 용량의 페이지를 적재하려고 할 때 (메모리가 꽉 찼을 때) 페이지 알고리즘을 통해 페이지를 교체하는 것  프로그램이 실행되면서 요구 페이지가 늘어나고 메모리가 가득 차서 페이지를 올릴 자리가 없을 때 특정 페이지(victim page)와 교체하는 것.  해당 페이지가 수정됬는지 아닌지를 확인(modifiy bit or dirty biy)해서 수정됬으면 디스크에도 수정을 해야하므로 수정되지 않은 페이지를 골라 교체함.victim page희생자 페이지. 메모리에 할당되어야 하는 페이지에게 자신의 자리를 내어주는 페이지임.그렇다면 희생자 페이지는 어떤 알고리즘을 통해 고를까?FIFO Algorithm  할당된지 오래된 페이지를 교체하는 것.  오래된 페이지가 얼마나 자주 사용될지도 모르는데 교체하는 것은 무모함.  페이지 부재율(page fault)가 높아지고 효율적이지 않음  페이지가 교체되어도 바로 부재처리(page fault handling) 되어 다시 메모리에 적재시키며 프로세스가 정상적으로 동작함.Optimal Algorithm  최적 알고리즘. 앞으로 가장 오랫동안 사용되지 않을 페이지를 찾아 교체하는 것.  프레임 수가 고정된 경우 가장 낮은 페이지 부재율을 보장함.  알고리즘 구현이 어려움LRU Alogirithm  가장 오랜 기간 사용되지 않은 페이지를 교체하는 것.  optimal algorithm은 미래를 내다보는 것이라면 least recently used algorithm은 과거를 보고 미래를 예측한다고 생각하면 됨. (최근에 사용되지 않았으면 나중에도 사용되지 않을 것)  실질적으로 사용 가능한 알고리즘]]></content>
      <categories>
        
          <category> TIL </category>
        
      </categories>
      <tags>
        
          <tag> virtual memory </tag>
        
          <tag> 가상 메모리 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Main Momery]]></title>
      <url>/til/2018/11/27/TIL-main-memory/</url>
      <content type="text"><![CDATA[TIL 카테고리의 글은 그날 배운 것을 정리하는 목적으로 포스팅합니다. 내용이 잘못되었다면 댓글로 피드백 부탁드립니다.메모리 계층 구조레지스터  CPU Core 내부에 존재하며 Core에서 연산을 수행할 때 직접 참조할 수 있는 유일한 기억장치Core 내부에도 몇 개 존재하지 않는 귀한 기억장치이며 보통 한 워드(프로세서가 처리할 수 있는 가장 자연스러운 데이터의 크기)가 레지스터 하나의 크기임.캐시 메모리  CPU Core 내부에 존재하며 Core와 입출력 버스를 통해 정보를 주고 받음명령어를 저장하는 Instruction Cache와 데이터를 저장하는 Data Cache로 구분되어있음.주로 Refresh (메모리의 내용을 보존하기 위해서 주기적으로 데이터를 갱신하는 작업. 이 작업이 수행중일 때는 메모리에 읽거나 쓰는 동작을 하지 못함)가 필요없는 SRAM(Static RAM)으로 구성함.CPU 내부에서 상당한 공간을 차지함.캐싱 (Caching)데이터를 느린 하위 계층(보조기억장치)의 데이터를 미리 가져다 놓는 작업캐싱할 데이터는 어떻게 선별할까?시간적 지역성 (Temporal Locality) : 최근 접근된 내용이 또 다시 접근될 확률이 높다.공간적 지역성 (Spatial Locality) : 한 지역의 내용이 접근되었다면, 그 주위의 내용도 접근될 확률이 높다.CPU 내부에 있는 캐시 컨트롤러는 위 두가지 원리를 적용해 캐시 메모리에 적재될 내용을 관리함.메인 메모리  CPU 외부에 존재하며 CPU와 North Bridge를 통한 데이터 버스를 통해 통신함.데이터를 보존하기 위해서 주기적으로 Refresh가 필요한 DRAM으로 구성됨.보조기억장치  하드디스크, CD, DVD, USB 메모리 등. 메인 메모리에 비해 접근 속도가 현저하게 떨어짐프로세서에서 접근 속도가 빠른 기억장치의 순서  레지스터  캐시 메모리  메인 메모리  보조 기억장치접근 속도가 빠르다 = 프로세서와 가깝다.메모리 관리Base Register, Limit Register운영체제는 각 프로세스들이 메모리에 올라왔을 때 다른 프로세스가 메모리 공간에 접근하는 것을 막아야함.이를 위해 메모리에서 프로세스의 시작 위치를 저장하는 Base register와 프로세스에게 할당된 메모리의 크기 Limit register 값을 이용해 프로세스들의 메모리 공간(space)의 경계를 나눔.Base Register &lt;= 프로세스가 접근할 수 있는 공간 &lt;= Base Register+ Limit RegisterAddress Binding  프로세스가 접근해야 하는 값과 함수에 대한 주소가 정해지는 것Logical Address, Virtual Address  논리적 주소. CPU가 생성하는 주소Physical Address  물리적 주소. 실제 접근해야하는 메모리 주소.  메모리에 나타나는 주소  Logical Address와 다를 수 있음.바인딩 시점      컴파일 시점 주소 바인딩          컴파일이 될 때 컴파일러에서 멍령어들과 변수의 메모리 주소가 정해지는 것              여기서 정해지는 주소는 실제 메모리 주소.      CPU에서 명령어를 실행할 때 이미 주소가 정해져있으므로 따로 주소 계산을 하지 않고 바로 접근해야하는 주소가 되므로 컴파일 시점 바인딩에서는 Logical Address  = Physical Address      주소가 결정나고 프로그램을 다른 시스템에서 실행했을 때 해당 시스템의 메모리를 어디서부터 하ㄹ당해주는지 나타내는 stating point가 다르거나 이미 그 주소에 다른 프로그램이 실행 중이라면  이 프로그램은 실행할 수 없음.            로드 시점 주소 바인딩          프로그램이 컴파일 되고 메모리에 올라갈 때 CPU에서 주소가 계산되는 것              컴파일 된 프로그램 내의 각 변수나 명령어의 주소는 특정 값을 기준으로 정해짐.      메모리에 올라가 CPU에서 주소를 계산할 때 프로그램이 올라간 메모리 주소의 Base Register 값에 이를 더하여 주소를 만들어냄.      CPU 내에서 계산되고 발생한 Logical 주소는 해당 메모리의 실제 주소 Physical Address와 같게됨.      한번 메모리에 올라가면 종료될 때 까지 그 위치에 존재하게됨.            실행 시점 주소 바인딩          메모리에 올라가 매번 명령어가 실행될 때 마다 주소가 계산되는 것              프로그램의 메모리 위치가 매번 변하기 때문에 이 방법을 사용.      프로세스는 여러 이유로 인해 메모리 공간 안에서 위치가 바뀔 수 있음. -&gt; 시작 위치는 계속 바뀌고 프로세스 내의 주소들도 바뀌게 되는데 위 두가지 방법으로는 매번 바뀌는 메모리 주소에 접근을 할 수 없게됨.      CPU에서 나온 임의의 주소 MMU를 걸쳐 실제 주소로 바뀌어 접근함.      CPU에서 발생한 주소가 Logical Address이고 MMU를 통해 바뀐 주소가 메모리의 실제 주소인 Physical Address이므로 두 주소가 다름 (Logical Address는 가상 주소)      MMU (Memory Management Unit)  가상 주소를 물리 주소로 매핑하는 하드웨어 장치  재배치  레지스터를 사용함.프로그램이 메모리에 올라가는 과정  로드과정          프로그램을 메모리에 올림        링킹 과정          라이브러리 파일과 연결해줌      메모리에 올라가서도 지속적으로 메모리 공간을 차지하는 것이 아닌 운영체제의 필요에 의해 swap-in되거나 swap-out 되기도 함linking  프로그래머가 작성한 소스코드를 컴파일하여 생성된 목적파일과 이미 컴파일된 라이브러리 파일들을 묶어 하나의 실행 파일을 생성하는 과정Dynamic Linking  프로그램이 실행되기까지 링킹이 지연됨.Static linking매번 프로세스 메모리에 사용하는 라이브러리를 확보하여 프로세스와 함께 올림. -&gt; 메모리 낭비다이나믹 링킹은 shared libraries라고도 불림.Swappingready queue에 있는 우선순위가 높은 프로세스가 메모리에 있는 프로세스와 자리를 바꾼다고 생각하면 됨.메모리에 있던 프로세스는 backing store라는 곳에 머물다가 다시 메모리에 할당되어 실행됨.메모리에서 backing store로 가는 것을 swap out이라고 하고 다시 메모리에 할당되는 것을 swap in이라고 함.메모리 할당 방법연속적인 할당프로세스에게 메모리를 연속적으로 할당하는 방법.hole(구멍)은 비워져있는 메모리 공간으로 프로세스에게 할당할 수 있는 메모리 공간(avaliable)임.  first-fit          최초로 할당받을 수 있는 크기의 공간에 무조건 할당됨.        best-fit          크기가 가장 근접한 공간에 할당됨. 속도가 가장 빠름.        worst-fit          크기가 가장 많이 차이가 나는 공간에 할당됨.      이 연속적인 할당 방법에는 단편화 문제가 생기는데 단편화에는 두 종류에 단편화가 있음.  외부 단편화 (external fragmentation)          메모리 공간에는 요청에 만족하는 크기의 할당할 수 있는 공간이 존재하나, 연속적이지 못해 할당할 수 없음.        내부 단편화 (internal fragmentation)          메모리를 할당해주는 단위가시스템의 block 단위인데, 만약 요청하는 메모리 크기가 14.5kByte이고 시스템의 block 단위가 1kByte이면 15kByte를 할당해주게 되고 이 때 0.5 kByte의 메모리가 낭비됨.      이 단편화 문제를 해결할 방법에는 압축과 비연속적인 할당이 있음.압축 방법의 경우 상당히 제한적임.비지속적인 할당Paging  논리 메모리는 page라 불리는 고정 크기의 블록으로 나누어지고 물리 메모리는 frame 이라는 페이지와 같은 크기의 블록들로 나누어짐.page와 frame를 매핑할 page table이 존재하는데 이 page table에는 각 페이지 번호와 그에 해당하는 frame의 시작 물리 주소를 저장함.Segmentation  페이징과 비슷하나 논리 메모리와 물리 메모리를 같은 크기의 블록이 아닌 서로 다른 크기의 논리적 단위인 segment로 분할함.블록의 크기를 모르기 때문에 segment table에는 해당 segment의 시작점인 base와 segment의 길이인 limit을 가지고 있음.refer  oprating system concepts 8장   http://baked-corn.tistory.com/16?category=718232  https://medium.com/@lyoungh2570/%EB%A9%94%EB%AA%A8%EB%A6%AC-%EA%B4%80%EB%A6%AC-e770ff60db8f  https://m.blog.naver.com/s2kiess/220149980093]]></content>
      <categories>
        
          <category> TIL </category>
        
      </categories>
      <tags>
        
          <tag> 메인 메모리 </tag>
        
          <tag> 메모리 관리 </tag>
        
          <tag> main memory </tag>
        
          <tag> memory management </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Deadlock]]></title>
      <url>/til/2018/11/24/TIL-deadlock/</url>
      <content type="text"><![CDATA[TIL 카테고리의 글은 그날 배운 것을 정리하는 목적으로 포스팅합니다. 내용이 잘못되었다면 댓글로 피드백 부탁드립니다.Deadlock (교착상태)  요청한 자원을 다른 프로세스가 점유하고 있고 점유하고있는 프로세스도 다른 자원에 대해 대기 상태에 있기 때문에 두 프로세스가 대기 상태에서 벗어날 수 없는 상황시스템 모델프로세스가 사용하는 자원(resource)에는 어떤 것들이 있을까?  request  use  release정상적인 프로세스는 다음 순서로만 자원을 사용할 수 있음.request프로세스는 자원을 요청하고, 즉시 허용되지 않는 경우 자원을 얻을 때 까지 대기상태에 놓임use프로세스는 자원에 대해 작업을 수행함.release프로세스가 자원을 다 사용했다면 방출함.이러한 경쟁 구도에 놓인 프로세스들은 자원을 요청하는 시점에 해당 자원이 다른 프로세스에 의해 점유되어있으면 대기 상태에 놓이고 각 프로세스와 자원들이 서로 꼬리를 물며 자원을 대기하는 경우를 교착상태에 놓여져있다고 함.교착상태 발생조건  상호배제 mutual exclusion          프로그램들이 공유 자원을 동시에 쓸 수 없는 상황        점유 상태로 대기          공유 자원을 점유한 상태에서 다른 자원을 기다리는 것        선점 불가          자원을 어떤 프로세스가 점유 중일 때 다른 프로세스가 그 자원을 뺏을 수 없는 것        순환성 대기          대기가 꼬리에 꼬리를 문 상황      교착상태가 발생하려면 위 4가지 조건이 반드시 성립되어야함.교착상태 처리방법  교착상태를 예방하거나 회피.  시스템이  교착상태가 되도록 허용한 다음 이를 회복시키는 방법.  시스템의 교착상태를 무시하고 발생하지 않는 것 처럼 꾸미는 방법(실제 OS에서 사용)교착상태 예방      상호배제 : 읽기 전용 같은 여러 프로세스가 동시에 접근할 수 있는 파일에는 상호배제가 필요하지 않기 때문에 상호배제가 깨어지게 되어 교착상태를 예방할 수 있음. but 공유 불가능한 데이터에서는 교착 상태를 예방하지 못하는 경우가 발생할 수 있음.        점유하며 대기 :  프로세스가 어떤 자원을 점유하고 있을 때 다른 자원을 요청하지 못하도록 보장해야함. 자기 자신이 아무런 자원을 점유하고 있지 않을 때에만 다른 자원을 요청할 수 있어야함.          프로토콜 1 : 프로세스가 실행되기 전에 자신이 필요한 모든 자원을 요청하여 할당 받는 것                  많은 자원들이 할당 된 후 오랫동안 사용되지 않아 자원의 이용률이 낮아짐.          필요한 자원 중 최소 하나가 계속 다른 프로세스에게 할당되어 있으면 이 프로세스는 무한정 대기해야하는 가능성이 생김.(기아상태)                    프로토콜 2 : 프로세스가 자원을 전혀 점유하지 않을 때만 자원을 요청할 수 있도록 하는 것                  자원을 동시에 사용해서 어떤 작업을 해야할 때, 비효율적인 동작이 나타나 자원의 이용률이 낮아짐                          비선점          이미 할당된 자원이 선점되지 않아야 한다는 것              어떤 자원을 점유하고있는 프로세스가 즉시 할당할 수 없는 다른 자원을 요청하면 자신이 점유하고 있는 자원을 강제로 방출시켜 다른 프로세스가 선점하게 함으로써 교착상태를 끊어버림.      해당 프로세스는 자신이 요청하고 있는 새로운 자원 + 강제로 방출한 옛 자원을 획득해야만 다시 시작됨            순환 대기 : 모든 자원 유형들에게 전체적인 순서를 부여하여 각 프로세스가 순서대로 자원을 요청하도록 강제함. 모든 자원들은 먼저 할당되는 순서가 정해져 있기 때문에 교착상태가 일어날 수 없음.  교착상태 예방 방법은 자원의 낭비가 심함.교착상태 회피(탐지)어떤 프로세스가 요청을 할 때 미래에 대한 분석을 통해 나의 요청을 늦추는 방법으로 교착상태를 피할 수 있음.  프로세스들이 필요로 하는 각 자원별 최대  사용량을 미리 선언함.  순환상태가 절대로 있을 수 없게함.안전상태 Safe State각 유효 자원의 최대 개수까지 어떤 순서로 요청을 하더라도 교착상태를 야기하지 않고 모두 할당을 잘 해줄 수 있음을 뜻함. 회피 알고리즘은 시스템이 정해진 최대 자원 내에서 시스템이 항상 안전 상태에 있도록 한함.오직 시스템이 안정상태로 유지될 수 있는 경우에만 즉시 요청을 들어줌.프로세스가 자원을 요구할 때 시스템은 자원을 할당한 후에도 안정 상태로 남아있게 되는지를 검사하여 교착 상태를 회피함.  banker's Algorithm  현재 할당 된 자원의 양(Allocation)을 계산하고 프로세스에게 최대로 할당될 수 있는 자원의 양(MAX)을 계산하고 두 데이터를 통해 추가로 할당해줄 수 있는 자원의 양(Need)을 계산함 (Need=Max-Allocation)  현재 가용할 수 있는 자원의 양(Avaliable)보다 Need 수보다 높으면 할당해주지 않고 요청한 정도가 Need보다 낮더라도 할당해주지 않음. http://asfirstalways.tistory.com/129교착상태 회복시스템이 교착상태가 발생하는 것을 허용하고 이를 회복함.교착상태 회복 방법  한개 이상의 프로세스를 중지  하나 이상의 프로세스들로부터 자원을 선점하는 것프로세스 종료  교착상태를 해결하기 위해 존재하는 프로세스 하나를 임의로 종료하여 교착 상태를 해결하는 방법  교착 상태  프로세스를 모두 중지          교착상태에 가기까지 프로세스들이 진행한 많은 작업들이 중지로 인해 결과가 폐기된다면 다시 계산을 해야하기 때문에 상당히 큰 비용이 들어감        교착 상태가 제거될 때 까지 한 프로세스씩 중지          각 프로세스가 중지될 때 마다 아직도 교착 상태에 있는지 매번 살펴봐야하기 때문에 상당한 오버헤드 유발.      자원 선점  교착 상태가 깨어질 때 까지 프로세스로부터 자원을 계속적으로 선점해 다른 프로세스에게 주어야함.자원 선점 고려 사항      희생자 선택 (selection of a victim)    자원 선점에 앞서 어떤 자원과 어느 프로세스가 선점될 것인가를 고민해야함. 비용을 최소하기 위해서 교착 상태 프로세스가 점유하고 있는 자원의 수, 교착상태 프로세스가 지금까지 실행하는 데 소요한 시간 등을 고려해 희생자를 선택함. minimize cost         롤백 (Rollback)          안전한 상태로 돌아가 해당 상태에 대한 프로세스를 다시 시작        특정 프로세의 자원을 강제로 방출하고 선점했다면 그 프로세스를 어떻게 처리할 것인가의 고민이 필요. 가장 안전한 방법은 프로세스를 중지시키고 재시작하는 것.(롤백)        기아 상태 (starvation)           동일한 프로세스를 항상 희생자로 선택할 수 있음 (평생 실행 X)        계속해서 특정 프로세스의 자원을 강제 방출시켜 선점을 시켜주게 되면 그 프로세스는 계속해서 희생자로 선택될 확률이 높고 그 프로세스는 영원히 실행이 완료되지 못하는 ㅠ_ㅠ 기아 상태에 빠질 수 있음. 그렇기 때문에 프로세스가 한정된 시간에만 희생자로 선정된다는 것을 보장해야함.  referOperating System Concepts             [운영체제      교착상태(deadlock)란 무엇인가?](https://frontalnh.github.io/2018/04/05/%EC%9A%B4%EC%98%81%EC%B2%B4%EC%A0%9C-%EA%B5%90%EC%B0%A9%EC%83%81%ED%83%9C-deadlock-%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80/#%EC%8B%9C%EC%8A%A4%ED%85%9C-%EB%AA%A8%EB%8D%B8)      [운영체제] 데드락, 교착상태 해결 (Dead lock)http://asfirstalways.tistory.com/129]]></content>
      <categories>
        
          <category> TIL </category>
        
      </categories>
      <tags>
        
          <tag> 교착상태 </tag>
        
          <tag> deadlock </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[프로세스 동기화]]></title>
      <url>/til/2018/11/20/TIL-process-synchronization/</url>
      <content type="text"><![CDATA[프로세스 동기화  여러 프로세스의 접근이 가능한 공유 데이터에 여러 프로세스가 동시에 접근해도 데이터의 일관성 (data consistency)을 유지하기 위한 개념  공유된 데이터에 동시에 여러 프로세스들이 접근했을 경우 예측하지 못한 일이 발생하는 경우를 배제하기 위함.  Data consistency를 보장해줘야함.Critical Section  상호 배제의 원리가 지켜져야 하는 영역 (임계영역)  공유 데이터에 접근하는 코드 블록  공유 데이터에 동시에 접근하면 데이터의 무결성(일관성)이 손상될 수 있음.Critical Section Problem  생산자-소비자 문제  생산자와 소비자 역할을 하는 프로세스 2개가 동시에 공유 자원에 접근함으로써 기대했던 값과는 다른 결과가 나옴.Race Condition  임계 영역의 부적절한 처리로 예상치 못한 결과를 발생시킬 수 있는 상황.  해소하기 위해선 하나의 프로세스만 임계 영역을 실행할 수 있도록 함. (mutual)임계구역의 3가지 요구 조건  mutual exclusion 상호배제          특정한 프로세스가 critical section에서 실행되는 동안 다른 프로세스가 접근할 수 없다.        process 진행          critical section을 사용하지 않고 있다면 다른 프로세스가 접근할 수 있도록 한다.        bounded waiting 한정된 대기          ciritical section 진입 횟수에 한계가 있어 같은 프로세스가 계속 독점해서 사용하지 못하게 한다.      다른 프로세스들이 기아 상태에 빠지지 않도록 한다.      deadlock  교착상태, 두개 이상의 작업이 서로 성대방의 작업이 끝나기만을 기다리고 있기 때문에 결과적으로 아무것도 완료되지 못하는 상태교착상태가 발생하는 조건      mutual exclusion    프로그램들이 공유 자원을 동시에 쓸 수 없는 상황.        점유 상태로 대기    공유 자원을 점유한 상태에서 다른 자원을 기다리는 것.        선점 불가    자원을 어떤 프로세스가 점유 중일 때 다른 프로세스가 그 자원을 뱃을 수 없다는 것        순환성 대기    대기가 꼬리게 꼬리를 문 상황  ]]></content>
      <categories>
        
          <category> TIL </category>
        
      </categories>
      <tags>
        
          <tag> 프로세스 동기화 </tag>
        
          <tag> process synchronization </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[힙]]></title>
      <url>/til/2018/11/19/TIL-heap/</url>
      <content type="text"><![CDATA[Heap  이진트리의 한 종류. 최댓값 및 최솟값을 찾아내는 연산을 빠르게 하기 위해 고안된 완전이진트리(Complete binary tree)를 기본으로 한 자료구조(tree-based structure)힙의 조건  루트 노드가 언제나 최댓 값(최대 힙) 또는 최솟 값(최소힙)을 가짐.  완전 이진 트리여야함. (leaf 노드 전의 level의  노드가 포화 이진트리여야함)  모든 노드에서 자신의 자식보다 큰 키(작은 키) 값을 가지고 있음.힙 정렬  자료구조 힙을 사용해 시간복잡도 최대 O(nlogn)을 가지는 정렬 알고리즘힙 정렬 과정  정렬되지 않은 리스트의 요소를 하나씩 힙에 insert 시간복잡도 O(logn)  모두 insert가 되면 하나씩 remove 하면서 새로운 리스트에 값을 insert 시간복잡도 O(logn)이 과정을 거치면 정렬된 리스트를 반환한다.def heap_sort_desc(unsorted):    """    MAX 힙을 활용한 정렬 알고리즘    힙 삽입의 시간 복잡도 O(logn)    삭제의 시간 복잡도 O(logn)    =&gt; O(nlogn) 복잡도    :return:    """    h = MaxHeap()    sorted = []    for data in unsorted:        h.insert(data)    d = h.remove()    while d:        sorted.append(d)        d = h.remove()    return sorted]]></content>
      <categories>
        
          <category> TIL </category>
        
      </categories>
      <tags>
        
          <tag> 힙 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[이진탐색트리]]></title>
      <url>/til/2018/11/17/TIL-BST/</url>
      <content type="text"><![CDATA[TIL 카테고리의 글은 그날 배운 것을 정리하는 목적으로 포스팅합니다. 내용이 잘못되었다면 댓글로 피드백 부탁드립니다.BST  모든 노드에 대해서 왼쪽 서브트리에 있는 데이터는 모두 현재 노드의 값보다 작고 오른쪽 서브트리에 있는 데이터는 모두 현재 노드의 값보다 큼.  값이 중복되는 것은 제외함  이진 탐색 트리 데이터의 표현          각 노드는 key, value의 쌍으로 이루어짐.      장점  데이터 원소의 추가, 삭제가 용이함.  요소 검색이 빠름(이진탐색)단점  공간 요소가 큼이진탐색(이진 검색 알고리즘)  이진 검색 알고리즘(binary search algorithm)은 오름차순으로 정렬된 리스트에서 특정한 값의 위치를 찾는 알고리즘이다. 처음 중간의 값을 임의의 값으로 선택하여, 그 값과 찾고자 하는 값의 크고 작음을 비교하는 방식을 채택하고 있다.이진 탐색 트리는 요소 검색에 매우 효율적인데 이 때 이진탐색을 사용함.이진탐색의 시간 복잡도는 O(logn)이며, 항상 시간복잡도가 O(logn)은 아님.그럼 언제 시간복잡도가 O(logn)이 아닐까?바로 이진 탐색 트리의 높이의 균형이 유지되어있지 않은 경우.위와 같이 생긴 이진 탐색 트리의 경우는 이진 탐색을 해도 효율적이지 않음. O(logn)이 아님.이진 탐색은 높이가 비슷하고 왼쪽 오른쪽의 균형이 맞을 때 효율적임.보다 성능이 좋은 이진 탐색 트리높이의 균형을 유지하는 이진 탐색 트리  AVL Tree  Red-black Tree위 트리들은 좋은 탐색 성능을 갖추기 위한 구조의 트리이지만 그만큼 삽입과 삭제 연산이 복잡하다.]]></content>
      <categories>
        
          <category> TIL </category>
        
      </categories>
      <tags>
        
          <tag> 이진탐색트리 </tag>
        
          <tag> Binary Search Tree </tag>
        
          <tag> 트리 </tag>
        
          <tag> BST </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Thread]]></title>
      <url>/til/2018/11/15/TIL-Thread/</url>
      <content type="text"><![CDATA[TIL 카테고리의 글은 그날 배운 것을 정리하는 목적으로 포스팅합니다. 내용이 잘못되었다면 댓글로 피드백 부탁드립니다.Thread  프로그램의 실행단위  한 프로세스 내에서 동작되는 여러 실행 흐름으로 프로세스 내의 주소 공간이나 자원을 공유할 수 있음  프로세스에 할당 된 자원을 씀!  각각의 스레드는 독립적인 작업을 수행해야 하기 때문에 각자의 스택과 PC Register를 가짐스레드의 구성  스레드 ID  프로그램 카운터  레지스터 집합  스택###멀티 스레딩  하나의 프로세스를 다수의 실행단위로 구분하여 자원을 공유하고 자원의 생성과 관리의 중복성을 최소화하여 수행능력을 향상시키는 것.여러 프로세스로 할 수 있는 작업들을 하나의 프로세스에서 스레드로 나누어 수행함.장점  프로세스를 이용하여 동시에 처리하던 일을 스레드로 구현할 경우 메모리 공간과 시스템 자원소모가 줄어듬  스레드간 통신이 필요한 경우에도 별도의 자원을 이용하는 것이 아닌 Heap 영역을 이용해 데이터를 주고받음.  스레드의 context switching은 캐시 메모리를 비울 필요가 없기 때문에 빠름.문제점      멀티 프로세싱 기반의 프로그래밍을 할 땐 프로세스간 공유하는 자원이 없어 동일한 자원에 동시에 접근하는 일이 없지만 멀티스레딩은 서로 다른 스레드가 데이터와 힙 영역을 공유하기 때문에 어떤 스레드가 다른 스레드에서 사용중인 변수나 자료구조에 접근해 엉뚱한 값을 읽어오거나 수정할 수 있음    해결    멀티 스레딩 환경에서는 동기화 작업이 필요함!          동기화를 통해 작업 처리 순서 컨트롤, 공유 자원에 대한 접근을 컨트롤 해야함. 이 때 병목현상이 발생해 성능이 저하될 가능성이 있기 때문에 과도한 락을 걸지 않도록 주의함.      referhttps://magi82.github.io/process-threadhttp://threestory.tistory.com/3]]></content>
      <categories>
        
          <category> TIL </category>
        
      </categories>
      <tags>
        
          <tag> Thread </tag>
        
          <tag> 쓰레드 </tag>
        
          <tag> 멀티스레딩 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[process란?]]></title>
      <url>/til/2018/11/14/TIL-Process/</url>
      <content type="text"><![CDATA[TIL 카테고리의 글은 그날 배운 것을 정리하는 목적으로 포스팅합니다. 내용이 잘못되었다면 댓글로 피드백 부탁드립니다.Process  실행 중인 프로그램으로 디스크로부터 메모리에 적재되어 CPU의 할당을 받을 수 있는 것운영체제로부터 주소 공간, 파일, 메모리 등을 할당 받으며 이것들을 총칭해 프로세스라고함.PCB (Process Control Block)  특정 프로세스에 대한 중요한 정보를 저장하고 있는 운영체제의 자료구조  운영체제는 프로세스를 관리하기 위해 프로세스의 생성과 동시에 고유한 PCB를 생성함.  프로세스는 CPU를 할당 받아 작업을 처리하다가 프로세스 전환이 팔생하면 진행하던 작업의 상황을 PCB에 저장한 후 CPU를 반환함. 그리고 다시 CPU를 할당 받게 되면 PCB에 저장되어있던 내용을 불러와 이전에 종료됬던 시점부터 다시 작업을 수행함.저장되는 정보  PID : 프로세스 식별 번호      프로세스 상태 : 대기 중인가? 실행 중인가? (new, ready, running, wating, terminated)    프로그램 카운터 : CPU가 다음에 실행할 명령어의 주소  프로그램 우선순위  CPU 레지스터  권한 : 프로세스가 접근할 수 있는 자원을 결정하는 정보  프로세스의 데이터와 명령어가 있는 메모리 위치를 가르키는 포인터 : 프로그램에 대한 정보를 가지고 있는 주소  실행문맥 : 프로세스가 실행 상태에서 마지막으로 실행한 프로세서의 레지스터 내용을 담고 있음.  등프로세스 관리운영체제는 프로세스들의 실행 사이에 프로세스를 교체하고 재시작할 때 오류가 발생하지 않도록 관리해야 하기 때문에프로세스들을 상태전이를 통해 체계적으로 관리함  디스패칭 : 프로세서가 사용 가능 상태가 되면 CPU를 할당 받음. 준비 상태에서 실행 상태로 상태전이 되는 것]]></content>
      <categories>
        
          <category> TIL </category>
        
      </categories>
      <tags>
        
          <tag> 프로세스 </tag>
        
          <tag> process </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[서버사이드 렌더링과 클라이언트사이드 렌더링]]></title>
      <url>/til/2018/11/13/TIL-SSR-CSR/</url>
      <content type="text"><![CDATA[TIL 카테고리의 글은 그날 배운 것을 정리하는 목적으로 포스팅합니다. 내용이 잘못되었다면 댓글로 피드백 부탁드립니다렌더링  어떠한 웹 페이지 접속 시 그 페이지를 화면에 그려주는 것서버사이드 렌더링(SSR)  View가 어떻게 보여질지 서버에서 해석해서 보내주는 것  서버 측에서 HTML &amp; View를 생성하여 응답하는 방법  웹 페이지 요청 시 새로고침이 일어나며 서버에 새로운 페이지에 대한 요청을 하는 방식클라이언트사이드 렌더링(CSR)  처음에 하나의 빈 페이지만 서버 측에서 제공하고 View에 대해서는 Client가 자바스크립트를 통해 렌더링하는 방식  클라이언트 측에서 View를 생성하는 방법SSR과 CSR의 차이점  초기 View 로딩 속도  SEO초기 View 로딩 속도SSR은 View를 서버에서 렌더링하여 가져오기 때문에 첫 로딩이 매우 짧음.CSR은 서버에서 View를 렌더하지 않고 HTML을 다운받은 다음 js파일이나 각종 리소스를 다시 렌더링하여 보여주기 때문에 SSR보다는 View를 보기까지 시간이 오래걸림. (로딩이 길어짐)SEO(Search Engine Optimization)CSR는 검색엔진 최적화 문제가 있음.CSR로 이루어지는 사이트는 View를 생성하는데 자바스크립트가 필요함!그 전까지는 HTML 내용이 비어있기 때문에 웹 크롤러들은 내용을 알 수 없고 제대로된 데이터를 수집할 수 없게됨.정리서버사이드 렌더링 (SSR)  초기 로딩 속도 빠름  SEO 유리  View 변경 시 서버에 계속 요청하므로 서버에 부담클라이언트사이드 렌더링(CSR)  초기 로딩 속도 느림  초기 로딩 후에는 서버에 다시 요청할 필요 없이 클라이언트 내에서 매우 빠름  SEO 문제가 있음SSR, CSR의 문제를 해결하는 방법첫번째 페이지 로드는 서버사이드 렌더링을 사용하고 그 후 모든 페이지 로드는 클라이언트사이드 렌더링을 사용한다.referhttp://asfirstalways.tistory.com/244http://jaroinside.tistory.com/24]]></content>
      <categories>
        
          <category> TIL </category>
        
      </categories>
      <tags>
        
          <tag> SSR </tag>
        
          <tag> CSR </tag>
        
          <tag> 서버사이드렌더링 </tag>
        
          <tag> 클라이언트사이드렌더링 </tag>
        
          <tag> 서버사이드렌더링 클라이언트사이드렌더링 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[git commit 합치기 (rebase -i)]]></title>
      <url>/git/2018/11/08/git-commit-%ED%95%A9%EC%B9%98%EA%B8%B0/</url>
      <content type="text"><![CDATA[상황회사에서 기능을 구현하고 풀리퀘까지 보낸 상황에서 로직에 수정해야하는 부분을 발견했다.로직을 수정하고 커밋하려고 하니 지저분한 커밋이 늘어나는 것 같아 굉장히 찝찝해졌다. ‘이건 커밋 메세지를 또 뭐라하지..’어떻게 하면 좋을 까 생각해보니 예전에 사수님께서 커밋을 합칠 수 있다고 말씀해주신 것이 생각나 바로 검색을 해보았다.커밋 합치기깃을 사용하는 사람이라면 같은 기능 구현에 대한 여러 번의 커밋을 한적이 있을 것이다.웹 디자인 css를 완성하고 '웹 디자인 구현'이라고 커밋을 했는데 웹 페이지를 다시 보니 폰트 사이즈가 맘에 안들어서 수정한 후 '폰트 사이즈 수정' 이라고 커밋을 했다.커밋 후 다시 보니 이번에는 폰트 색깔이 맘에 안들어 바꾸고 '폰트 색깔 수정' 이라고 커밋을 했다.디자인을 구현한건 똑같은데 여러 번의 커밋이 이루어졌다.이렇게 되면 협업을 할 때 다른사람이 커밋의 내용에 대해 의문이 들 수도 있고 이런 커밋들이 많아지면 커밋 이력은 매우 지저분해진다.이럴 때 커밋을 합치면 이런 문제를 해결할 수 있다.커밋을 합치는 것은 rebase -i 명령어로 합칠 수 있다.먼저 git log를 통해 내가 어디서부터 어디까지 커밋을 합칠 것인지 살펴보자.commit e1f09108a51c09919c3488f06e3160c8f57Author: Date:   Thu Nov 8 15:06:01 2018 +0900    Message 4commit 405715479d9e97bbe714820dda88e04dbcAuthor: Date:   Thu Oct 25 12:14:21 2018 +0900    Message 3Message 3 커밋에  Message 4 커밋을 합쳐보도록 하겠다.$ git rebase -i HEAD~2 위 명령어는 최근 커밋 2개를 합치라는 명령어이다.head는 현재 브랜치에서 마지막 커밋을 가르킨다.즉 마지막 커밋과 그 전 커밋을 합치는 것이다.만약 2개 커밋이 아닌 3개 커밋을 합치고 싶다면 HEAD~3 이라고 숫자만 바꿔주면 된다.위 명령어를 실행하면 vi 편집기가 뜰 것이다.pick 4057154 Message 3pick e1f0910 Message 4vi 상단에는 내가 합치고 싶은 커밋들이 나와있을 건데 여기서 합치고 싶은 커밋의 pick을 squash로 바꿔준다.pick 4057154 Message 3squash e1f0910 Message 4message 4 커밋을 message 3 커밋으로 합칠 거기 때문에 Message 4의 pick을 squash로 바꿔준다.:wq 저장을 하면# This is a combination of 2 commits.# This is the 1st commit message:Message 3# This is the commit message #2:# Message 4# Please enter the commit message for your changes. Lines starting이렇게 다시 vi 편집기가 뜨는데 여기서는 어떤 커밋 메세지를 사용할 것이냐를 묻는 것이므로Message 4 커밋 메세지를 지우면 message 3의 커밋 메세지만 남게 된다.지우지 않고 위와 같이 주석 처리를 할 수 있다.위 설정까지 다 했다면 커밋이 합쳐졌을 것이다.commit 405715479d9e97bbe714820dda88e04dbc493282Author: Date:   Thu Oct 25 12:14:21 2018 +0900    Message 3커밋 합친 것을 원격 저장소에도 반영하고 싶으면 push를 하면 된다.$ git push -f origin [branch]커밋 합치기 참 쉽죵? 🤗]]></content>
      <categories>
        
          <category> git </category>
        
      </categories>
      <tags>
        
          <tag> git rebase </tag>
        
          <tag> git commit 합치기 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Docker]]></title>
      <url>/til/2018/11/06/TIL-Docker/</url>
      <content type="text"><![CDATA[도커란?  컨테이너 기반의 오픈소스 가상화 플랫폼컨테이너격리된 공간에서 프로세스가 동작하는 기술 (가상화 기술)이미지를 실행한 상태.이미지컨테이너 실행에 필요한 파일과 설정 값등을 포함하고 있는 것.상태 값을 가지지 않고 변하지 않음.  이미지가 클래스이면 컨테이너는 클래스의 인스턴스이다.같은 이미지에서 여러 개의 컨테이너를 생성할 수 있고 컨테이너의 상태가 바뀌거나 삭제되더라도 이미지는 변하지 않고 그대로 남아있음.  하나의 서버에 여러개의 컨테이너를 실행하면 서로 영향을 미치지 않고 독립적으로 실행되어 마치 가벼운 VMVirtual Machine을 사용하는 느낌을 줍니다. 실행중인 컨테이너에 접속하여 명령어를 입력할 수 있고 apt-get이나 yum으로 패키지를 설치할 수 있으며 사용자도 추가하고 여러개의 프로세스를 백그라운드로 실행할 수도 있습니다. CPU나 메모리 사용량을 제한할 수 있고 호스트의 특정 포트와 연결하거나 호스트의 특정 디렉토리를 내부 디렉토리인 것처럼 사용할 수도 있습니다.정말 가벼운 VM 사용하는 것 같음! 짱Mac에서 도커 설치Docker-for-Mac 이 곳에서 로그인 후 설치!설치가 완료되었다면 도커가 잘 설치되었는지 확인해보자.$ docker versionClient: Version:           18.06.1-ce API version:       1.38 Go version:        go1.10.3 Git commit:        e68fc7a Built:             Tue Aug 21 17:21:31 2018 OS/Arch:           darwin/amd64 Experimental:      falseServer: Engine:  Version:          18.06.1-ce  API version:      1.38 (minimum version 1.12)  Go version:       go1.10.3  Git commit:       e68fc7a  Built:            Tue Aug 21 17:29:02 2018  OS/Arch:          linux/amd64  Experimental:     true명령어컨테이너 실행$ docker run ubuntu:16.04run ubuntu 이미지가 있으면 docker 컨테이너 실행, 없으면 이미지 다운로드하고 컨테이너 실행 후 바로 종료됨.$ docker run -rm -it ubuntu:16.04 /bin/bash-rm 컨테이너 실행 후 종료시 컨테이너 삭제 옵션-it  키보드 입력을 위한 옵션$ docker run -d -p 1234:6379 redis# 아래와 같이 접속$ telnet localhost 1234-d 백그라운드 작업-p 컨테이너 포트를 호스트 포트로 연결$ docker run -d -p 3306:3306 -e MYSQL_ALLOW_EMPTY_PASSWORD=true --name mysql mysql:5.7-e 환경변수 설정--name 컨테이너 이름 설정컨테이너 목록 확인$ docker ps # 실행 중인 컨테이너 목록$ docker ps -a # 전체 컨테이너 목록실행 중인 컨테이너 중지$ docker stop ${container_id or container_name}컨테이너 완전 삭제$ docker stop ${container_id or container_name}이미지 목록 확인$ docker images이미지 업데이트$ docker pull ubuntu:14.04 pull  이미지를 최신 버전으로 다시 다운 받을 수 있음.이미지 삭제$ docker rmi ${image_id}컨테이너가 실행 중인 이미지는 삭제되지 않음!컨테이너 실행 로그 확인$ docker logs ${container_id or container_name}컨테이너에서 실행한 로그를 확인.$ docker logs --tail 10 ${container_id or container_name}--tail컨테이너에서 실행한 로그 마지막 10줄까지 출력.$ docker logs -f ${container_id or container_name}-f실시간으로 로그가 생성되는 것을 확인함.컨테이너 명령어 실행$ docker exec -it mysql /bin/bash # 컨테이너에 접속$ mysql -uroot 컨테이너에 들어가 보거나 컨테이너의 파일을 실행하고 싶을 때.run 새로 컨테이너를 만들어서 실행.exec 실행 중인 컨테이너에 명령어를 내림.접속한 이후에는 어떤 작업도 할 수 있고 컨테이너를 마음껏 조작 할 수 있음.그 외컨테이너를 삭제한다는 것  컨테이너에서 생성된 파일이 사라지는 것  데이터베이스의 데이터가 사라지는 것  웹 애플리케이션의 사용자가 올린 이미지가 사라지는 것컨테이너 삭제시 유지해야하는 데이터는 반드시 컨테이너 내부가 아닌 외부 스토리지에 저장해야함.Docker compose설정 파일을 이용한 도커 설정version: '2'services:   db:     image: mysql:5.7     volumes:       - db_data:/var/lib/mysql     restart: always     environment:       MYSQL_ROOT_PASSWORD: wordpress       MYSQL_DATABASE: wordpress       MYSQL_USER: wordpress       MYSQL_PASSWORD: wordpress   wordpress:     depends_on:       - db     image: wordpress:latest     volumes:       - wp_data:/var/www/html     ports:       - "8000:80"     restart: always     environment:       WORDPRESS_DB_HOST: db:3306       WORDPRESS_DB_PASSWORD: wordpressvolumes:    db_data:    wp_data:referhttps://subicura.com/2017/01/19/docker-guide-for-beginners-2.html]]></content>
      <categories>
        
          <category> TIL </category>
        
      </categories>
      <tags>
        
          <tag> docker </tag>
        
          <tag> docker 사용법 </tag>
        
          <tag> docker 명령어 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[레드햇 페도라에서 Nginx 설치하기]]></title>
      <url>/nginx/2018/11/05/%EB%A0%88%EB%93%9C%ED%96%87-%ED%8E%98%EB%8F%84%EB%9D%BC%EC%97%90%EC%84%9C-Nginx-%EC%84%A4%EC%B9%98%ED%95%98%EA%B8%B0/</url>
      <content type="text"><![CDATA[레드햇 페도라에서 Nginx 웹 서버를 설치한다. centos OS에서도 똑같이 설치할 수 있다.먼저 nginx를 받을 repository를 추가한다.$ vi /etc/yum.repos.d/nginx.repo[nginx]name=nginx repobaseurl=http://nginx.org/packages/[OS]/[OSRELEASE]/$basearch/gpgcheck=0enabled=1여기서 대괄호로 감싸진 OS와 OSRELEASE를 자신의 OS와 버전에 따라 변경을 해줘야 한다.http://nginx.org/packages/ 여기에 있는 자신의 OS와 OS 버전에 맞는 것이 있는지 확인 후 변경을 해준다. (없으면 패키지 설치가 안 되기 때문에 꼭 확인하고 변경해주자)설정 후 저장을 하고 Nginx를 설치한다.$ yum install nginxNginx 명령어      Nginx 시작    $ sudo service nginx start        Nginx 중지    $ sudo service nginx stop        Nginx 재시작    $ sudo service nginx restart        Nginx 상태    $ sudo service nginx status        Nginx 설정 후 다시 로드    $ sudo service nginx reload        Nginx 설정이 성공 적인지 확인    $ sudo nginx -t  referCentOS 7 Nginx 설치하기]]></content>
      <categories>
        
          <category> Nginx </category>
        
      </categories>
      <tags>
        
          <tag> nginx 설치 </tag>
        
          <tag> fedora nginx 설치 </tag>
        
          <tag> 레드햇 페도라 nginx 설치 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[AWS(rhel fedora)Nginx-uWSGI-Django 연동하기]]></title>
      <url>/django/2018/11/05/Nginx-uWSGI-Django/</url>
      <content type="text"><![CDATA[이 포스팅에서는 AWS에서 서버를 셋팅할 것이기 때문에 AWS EC2 설정이 되어있다는 가정하에 진행한다.또한 글쓴이의 OS 환경은 레드햇 페도라 환경에서 진행했다.먼저 간단하게 서버 구조를 말하면  Nginx 80번 포트로 접속하면 uWSGI 8000번 포트로 요청을 보내 우리가 만든 Django 프로젝트에 접속하게 될 것이다.우리가 사용자에게 listen할 포트(80)는 열어놔야하기 때문에 AWS 사용자 지정 포트에서 80번 포트를 열어줘야한다.포트를 변경하고 싶으면 변경해도 되지만 설정한 포트에따라 사용자 지정 포트를 열어줘야 하는 것도 명심하자.1. Djangodjango 프로젝트는 배포될 준비가 다 되어있다는 가정 하에 진행한다.      가상환경 셋팅    static root 설정  media root 설정  $ python manage.py collectstatic2.Nginx글쓴이는 Nginx 설정에 정말 삽질을 했다.블로그를 보며 배포를 하려고 하는데 블로그에서는 sites-available 폴더에 conf 파일을 넣으면 된다고하는데.. 해당 폴더가없어서 삽질을 엄청했다.이 포스팅은 sites-available, sites-enabled 폴더가 없는 개발자들에게 유용한 포스트가 될 것이다.(그러길 바란다..)먼저 uWSGI와 nginx 서버 등을 설정하기 위해 conf 파일을 생성한다.etc/nginx/conf.d/testprj.conf 생성 (nginx의 경로는 OS 별로 다를 수  있기 때문에 꼭 확인해보자.)[..] 대괄호 안에있는 것은 자신의 프로젝트의 이름이나 경로에 따라 알맞게 변경한다.# etc/nginx/conf.d/testprj.conf server {    listen                   80; # nginx가 listen할 포트    server_name              [www.your.domain.com 123.23.56.5] #서버 도메인이나 아이피 입력    client_max_body_size     10M;    access_log               /var/log/testprj.access.log; # 성공 로그를 기록할 파일    error_log                /var/log/testprj.error.log; # 에러 로그를 기록할 파일     # -------------    # Handle Django    # -------------    location / {    	# 외부에서 특정 포트로 Nginx를 통해 http 요청을 받았을 때 요청을 uWSGI를 통해 Django로 넘김        proxy_pass       http://localhost:8000; # uWSGI가 nginx 요청을 받을 주소와 포트        proxy_set_header Upgrade            $http_upgrade;        proxy_set_header Connection         "upgrade";        proxy_set_header Host               $host;        proxy_set_header X-Real-IP          $remote_addr;        proxy_set_header X-Forwarded-For    $proxy_add_x_forwarded_for;        proxy_set_header X-Forwarded-Proto  $scheme;    }    # ------------------    # serve static files    # ------------------    # here we assume the STATIC_ROOT inside your django project is    # set to /static/    location /static/ {    	# /static/ 으로 들어 올 때 경로 설정.        alias   [/home/user/testprj/static/]; # django project settings.py에 설정되어있는 STATIC_ROOT 주소    }}여기서$ sudo nginx -tnginx: [emerg] could not build server_names_hash, you should increase server_names_hash_bucket_size: 64nginx: configuration file /etc/nginx/nginx.conf test failednginx 서버 확인 시 이런 오류가 난다면nginx.conf 파일 수정을 해줘야 한다.아래 설정을 http {} block에 설정 안에 넣는다.# etc/nginx/nginx.confhtttp{    server_names_hash_bucket_size 512;	server_names_hash_max_size 512;	...}오류에는 64라고 되어있어 64로 했더니 오류가 사라지지 않아  512로하니까 되었다.이렇게 하고 $ sudo nginx -t 수행 -&gt; 성공하면 nginx 설정 끝!3. uWSGI가상환경이 활성화된 상태에서 uWSGI 설치$ pip install uwsgi잘 깔렸는지 확인해보자$ wsgi --http :8000 --module [your-project-name].wsgi (manage.py 디렉토리에서)잘 되면 ini파일 생성$ sudo vi [testprj].ini# home/user/testprj/[testprj].ini[uwsgi] module          =  [testprj].wsgi:applicationmaster          =  truepidfile         =  [testprj].uwsgi.pidenable-threads  = true# uWSGI 포트 설정 http            =  127.0.0.1:8000 processes       =  5harakiri        =  50max-requests    =  5000# clear environment on exitvacuum          =  true# 가상환경 설정home            =  [/home/user/.pyenv/versions/3.6.5/envs/devenv]# background the processdaemonize       =  [testprj].uwsgi.log이렇게 하구 저장!  실행 $ uwsgi testprj.ini (가상환경 안에서 )  uWSGI 로그확인 $ vi testprj.uwsgi.log  uWSGI 중지 $ uwsgi --stop [testprj].uwsgi.piduWSGI 가 실행된 상태에서 nginx 서버를 시작한다.$ sudo service nginx start이후 맨 처음 nginx에서 생성한 conf 파일의 server_name에 설정한 주소(AWS 주소)로 들어가보면 Django project가 띄어져 있을 것이다! 그러면 연동 끝!설정 중 오류가 있으면 댓글 남겨주세요 💁‍referhttps://medium.com/@charlesthk/deploy-nginx-django-uwsgi-on-aws-ec2-amazon-linux-517a683163c6https://www.savour-it.com/posts/2018-02-06-nginx-uwsgi-django-setting/https://www.digitalocean.com/community/tutorials/how-to-serve-django-applications-with-uwsgi-and-nginx-on-ubuntu-14-04https://blog.leop0ld.org/posts/use-python3-django-uwsgi-nginx/http://charles.lescampeurs.org/2008/11/14/fix-nginx-increase-server_names_hash_bucket_size]]></content>
      <categories>
        
          <category> django </category>
        
      </categories>
      <tags>
        
          <tag> django </tag>
        
          <tag> nginx uwsgi django 연동 </tag>
        
          <tag> nginx uwsgi django </tag>
        
          <tag> aws nginx django </tag>
        
          <tag> aws django 배포 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Django-MySQL 연동하기]]></title>
      <url>/django/2018/11/05/Django-MySQL-%EC%97%B0%EB%8F%99%ED%95%98%EA%B8%B0/</url>
      <content type="text"><![CDATA[이 포스팅은 mac과 linux 환경에서의 Django-MySQL 연동 글이다.윈도우 환경에서 작업을 진행한다면, 이 포스팅 과 함께 보기 추천한다. (별로 다른 건 없지만 윈도우 환경에서 오류 해결 법이 좀 더 추가되어있다)MySQL이 설치되어있다는 가정하에 진행하므로 꼭 설치 후 아래 항목들을 차례로 진행하길 바란다. (설치가 되어있지 않으면 오류가 난다)Django와 MySQL을 연동하기 위한 연동 드라이버를 설치django에서 제공하는 MySQL 연동 드라이버 모듈은 3가지가 있다.  MySQLdb - 제일 안정된 드라이버, python3은 지원하지 않음  Mysqlclient - MySQLdb를 개선한 패키지, Python3.3 이상의 버전도 지원하고있음, 장고에서 추천함  MySQL Connector/Python - MySQL 개발사 오라클에서 제공하는 드라이버위 모듈 중 자신에게 맞는 드라이버를 설치하면 되는데 우리는 mysqlclient를 사용할 것이다.mysqlclient를 설치해준다.$ pip sintall mysqlclientcentos/rhel fedora OS에서 설치시mysqlclient OSError: mysql_config not found이런 오류가 발생한다면 mysql-devel을 설치해주면 해결된다.$ sudo yum install mysql-develDjango에서 MySQL 연동 세팅정상적으로 설치가 되었다면 django project의 settings.py 파일에서 DATABASE 항목을 설정해준다.# settings.pyDATABASES = {	'default': {		'ENGINE':'django.db.backends.mysql', # mysql 엔진 설정		'NAME':'mysite', # 데이터베이스 이름 		'USER':'root', # 데이터베이스 연결시 사용할 유저 이름		'PASSWORD':'password',# 유저 패스워드        'HOST':'l27.0.0.1', # 데이터베이스 서버 주소        'PORT':'3306' # 데이터베이스 서버 포트    }}ENGINE을 제외한 항목은 자신의 mysql 설정에 맞게 수정해야한다.지금까지 설정이 올바르게 되었다면 Django와 MySQL이 연동이 되었을 것이다.MySQL의 데이터(테이블)를 반영만약 현재 MySQL의 데이터베이스에 저장되어있는 테이블들을 Django project에 반영하고 싶다면$ python manage.py inspectdb &gt; ./app/models.py명령어를 사용하면 된다.mysql&gt; desc board_tbl;+---------+--------------+------+-----+---------+----------------+| Field   | Type         | Null | Key | Default | Extra          |+---------+--------------+------+-----+---------+----------------+| idx     | int(6)       | NO   | PRI | NULL    | auto_increment || writer  | int(11)      | YES  |     | NULL    |                || subject | varchar(255) | YES  |     | NULL    |                || content | text         | YES  |     | NULL    |                || date    | datetime     | YES  |     | NULL    |                |+---------+--------------+------+-----+---------+----------------+MySQL 테이블이 model로 적용된 모습# app/models.py...class BoardTbl(models.Model):    idx = models.AutoField(primary_key=True)    writer = models.IntegerField(blank=True, null=True)    subject = models.CharField(max_length=255, blank=True, null=True)    content = models.TextField(blank=True, null=True)    date = models.DateTimeField(blank=True, null=True)    class Meta:        managed = False        db_table = 'board_tbl'Django migration을 MySQL에 반영Django의 migration을 MySQL에 반영하는 것은 sqllite3에 migration을 반영하는 것과 같다. $ python manage.py migrate이 명령어를 실행하게 되면 MySQL 테이블에 auth_user 등 장고의 기본 테이블들이 생성된 것을 확인할 수  있을 것이다.이렇게 마지막까지 잘 되었다면 MySQL을 사용할 준비를 마친 것이다.이제 ORM, raw SQL 등을 기존 sqlite3 처럼 사용할 수 있다.]]></content>
      <categories>
        
          <category> django </category>
        
      </categories>
      <tags>
        
          <tag> django mysql연동 </tag>
        
          <tag> django mysql </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[GIL을 알아보자]]></title>
      <url>/python/2018/10/24/GIL/</url>
      <content type="text"><![CDATA[Global Interpreter Lock한  CPU만 일함.!다중 CPU 환경에서 동시에 여러 파이썬 코드를 병렬로 실행할 수 없음.  인터프리터의 구현이 쉬워짐  Garbage Collector 만들기 조음  C/C++ 확장 만들기 만들기 쉬움.프로그램에 I/O 작업을 많이하면 파이썬 쓰레드도 좋음.우리가 하는 일반 프로그래밍은 I/O Bound계산을 많이 하는 작업쓰레드 -&gt; 프로세스다중 CPU에서 병렬 실행을 위해서는 다중 프로세스를 이용하는 multiprocessing 모듈을 사용함.프로그램에 I/O 작업을 많이하면 파이썬 쓰레드도 좋음.우리가 하는 일반 프로그래밍은 I/O Bound계산을 많이 하는 작업쓰레드 -&gt; 프로세스다중 CPU에서 병렬 실행을 위해서는 다중 프로세스를 이용하는 multiprocessing 모듈을 사용함.th1 = Thread(target=do_work)th1.start()th1.join()pr1 = Process(target=do_work)pr1.start()pr1.join()Process운영체제가 작업하는 단위Thread이 Process 안에서 공유되는 메모리를 바탕으로여러 작업을 생성하는 작업 단위.-&gt; 각 thread마다 할단된 개인적인 메모리가 있으면서 thread가 속한 process가 가지는 메모리에도접근가능함.Race condition여러 thread가 공유된 데이터를 변경함으로써 발생하는 문제.“Thread-safe 하다”thread들이 race condition을 발생시키지 않으면서 각자의 일을 수행한다는 뜻.Mutex  Thread-safe한 코드를 만들기 위해서 사용하는 것 중 하나.  Race condition을 막기 위해 공유되는 메모리의 데이터를 여러 thread가 동시에 사용할 수 없도록 잠그는일을 맡음.휴대폰이 없던 시절에는 공중 전화를 주로 이용했었다. 거리의 모든 남자들은 각자의 아내에게 전화를 너무나 걸고 싶어한다.어떤 한 남자가 처음으로 공중 전화 부스에 들어가서 그의 사랑하는 아내에게 전화를 걸었다면, 그는 꼭 전화 부스의 문을 꼭 잡고 있어야 한다. 왜냐하면 사랑에 눈이 먼 다른 남자들이 전화를 걸기 위해 시도때도 없이 달려들고 있기 때문이다. 줄 서는 질서 문화 따위는 없다. 심지어 그 문을 놓친다면, 전화 부스에 들이닥친 남자들이 수화기를 뺏어 당신의 아내에게 애정 표현을 할 지도 모른다.아내와의 즐거운 통화를 무사히 마쳤다면, 이제 문을 잡고 있던 손을 놓고 부스 밖으로 나가면 된다. 그러면 공중 전화를 쓰기 위해 달려드는 다른 남자들 중 제일 빠른 한 명이 부스에 들어가서 똑같이 문을 꼭 잡고 그의 아내와 통화할 수 있다.  thread: 각 남자들  mutex: 공중 전화 부스의 문  lock: 그 문을 잡고 있는 남자의 손  resource: 공중 전화CPython reference counting 을 하는 과정에서Rece condition이 일어나면 그 결과는 메모리 유실/object가 죽음이 발생이를 해결하기 위해 mutex사용 -&gt; object 하나하나 마다 대응하는 mutex가 필요함! 성능적으로 많은 손해, deadlock 위험상황CPython의 결정mutex를 통해 모든 reference 개수를 일일히 보호하지 말고 python interpreter자체를 잠구자!-&gt; 오직 한 thread만이 python 코드를 사용할 수 있음한 프로세스 안에서 여러 쓰레드를 이용한 병렬처리를 막음.한 쓰레드가 파이썬코드를 실행하기 위해선 공중전화 박스에 들어가 interpreter lock을 잡아야함!그래서 Global Interpreter Lock]]></content>
      <categories>
        
          <category> python </category>
        
      </categories>
      <tags>
        
          <tag> python gil </tag>
        
          <tag> gil </tag>
        
          <tag> global interpreter block </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[heroku를 사용해 django 프로젝트 배포하기]]></title>
      <url>/django/2018/10/24/Django-heroku-deploy/</url>
      <content type="text"><![CDATA[git이 설치되어있다고 가정한다.1. heroku 회원가입2. 헤로쿠 사용을 위한 패키지 다운virtualenv 활성화 상태에서 패키지 다운로드$ pip install dj-database-url gunicorn whitenoise3. requirements.txt 파일 생성$ pip freeze &gt; requirements.txt생성된 requirements.txt 맨 아래에 아래 내용 추가$ psycopg2==2.7.14. Procfile 파일 생성헤로쿠에게 웹 사이트를 시작시키기 위해 실행되어야할 명령어의 순서를 알려주기 위해 procfile 파일 생성 (확장자 x, cmd에서 vi나 nano로 생성)web: gunicorn &lt;mysite&gt;.wsgi --log-file -웹 애플리케이션을 배포할 때 gunicorn &lt;mysite&gt;.wsgi 명령을 실행하는 것을 의미(gunicorn은 강력한 버전의 runserver 명령어)5. runtime.txt 파일 생성헤로쿠에게 어떤 버전의 파이썬을 사용하는지 알려줌.# djangoproject/runtime.txtpython-3.6.5 # 프로젝트 파이썬 버전5. settings.py 설정로컬 컴퓨터 설정 파일 local_settigns.py생성# local_settings.pyimport osBASE_DIR = os.path.dirname(os.path.dirname(__file__))DATABASES = {    'default': {        'ENGINE': 'django.db.backends.sqlite3',        'NAME': os.path.join(BASE_DIR, 'db.sqlite3'),    }}DEBUG = True이 파일은 로컬에서 프로젝트를 돌릴 때 사용을 하기 위함인 것 같다.6. 헤로쿠 배포를 위한 settings.py 설정# settings.pyimport dj_database_url...DEBUG = FalseALLOWED_HOSTS = ['127.0.0.1', '.herokuapp.com']...DATABASES = {    'default': {        'ENGINE': 'django.db.backends.postgresql_psycopg2',        'NAME': 'lowell',        'USER': 'name',        'PASSWORD': '',        'HOST': 'localhost',        'PORT': '',    }}...db_from_env = dj_database_url.config(conn_max_age=500)DATABASES['default'].update(db_from_env)7. wsgi.py 설정프로젝트의 wsgi.py 파일의 끝에 다음 라인 추가# wsgi.py...from whitenoise.django import DjangoWhiteNoiseapplication = DjangoWhiteNoise(application)8. gitignore 파일 생성*.pycdb.sqlite3myvenv__pycache__local_settings.py # 로컬 환경을 위한 파일이기 때문에 등록9. heroku 로그인아까 만들었던 계정으로 로긘하깅$ heroku login10. git 저장소 생성,커밋현재 장고 프로젝트 루트 디렉토리에 git 저장소를 생성부터 커밋$ git init$ git add . $ git status $ git commit -m "additional files and changes for Heroku"11. 애플리케이션 이름 설정$ heroku create &lt;appname&gt;이 이름은 도메인이 된다. &lt;appname&gt;.herokuapp.com12. 이제 배포하면 된다.$ git push heroku master13. 애플리케이션 접속헤로쿠에 웹 프로세스를 시작하라고 말한다.$ heroku ps:scale web=1앱에 들어가보장. 아래 명령어를 입력하면 브라우저가 뜰 것이당.$ heroku open14. migrate , createsuperuser$ heroku run python manage.py migrate$ heroku run python manage.py createsuperuser오류 발생시오류가 생겼다면 다음 블로그를 참조해보자.오류해결referhttps://tutorial-extensions.djangogirls.org/ko/heroku/http://ggilrong.tistory.com/entry/heroku%EC%97%90-django-%EC%95%B1%EB%B0%B0%ED%8F%AC%ED%95%98%EA%B8%B0http://morningbird.tistory.com/23]]></content>
      <categories>
        
          <category> django </category>
        
      </categories>
      <tags>
        
          <tag> django </tag>
        
          <tag> django 배포 </tag>
        
          <tag> django heroku </tag>
        
          <tag> heroku </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[python3 venv 가상환경 사용하기]]></title>
      <url>/python/2018/08/23/python3-venv-%EA%B0%80%EC%83%81%ED%99%98%EA%B2%BD-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0/</url>
      <content type="text"><![CDATA[venv는 python3에서 기본으로 제공하는 가상환경(virtualenv)을 만들 수 있는 라이브러리이다.나는 이 라이브러리를 여태동안 모르고 있었다!원래 mac에서는 pyenv를 사용하여 가상환경을 세팅했는데 윈도우는pyenv를 사용할 수 없다.그래서 그동안 서드파티 라이브러리인 virtualenv를 사용하고 있었는데 venv로 갈아탈 예정이다.기본으로 제공하는데 사용해줘야지!만약 virtualenv 라이브러리를 사용하고 싶다면 해당 포스트에서 사용법을 확인하면 된다.venv 사용하기먼저 venv는 python3에 빌트인 되어있기 때문에 설치를 하지 않아도 사용가능하고 python2.7에서는 사용이 불가능하다.python2를 사용한다면 virtualenv/pyenv 라이브러리를 설치해야한다.      가상환경 생성    # window # python -m venv [venv 이름]&gt; python -m venv ./myenv   # mac# python3 -m venv [venv이름]$ python3 -m venv ./myenv            가상환경 활성화    # window&gt; myenv\Scripts\activate# mac$ source myenv\bin\activate # or . myenv\bin\activate        가상환경에 성공적으로 들어간다면 prompt 앞에 현재 가상환경이 표시될 것이다.    지금부터는 격리된 환경에서 작업이 가능하다!    (myenv) $ &gt;        가상환경 비활성화    &gt; deactivate      ]]></content>
      <categories>
        
          <category> python </category>
        
      </categories>
      <tags>
        
          <tag> python virtualenv </tag>
        
          <tag> python venv </tag>
        
          <tag> python pyenv </tag>
        
          <tag> window pyenv </tag>
        
          <tag> python 가상환경 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
</search>
